{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjIw6aznmbpA"
      },
      "source": [
        "# Generative AI project about Data Augmentation\n",
        "> *This is the notebook for the ninth Profession AI project about Generative AI module*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKM3dG8tmbpB"
      },
      "source": [
        "## Setup & Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMOFpywHmjlA",
        "outputId": "bba60265-03fe-481e-afc3-9bbe62d411af"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Silvano315/Gen-AI-for-Data-Augmentation.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3uLuJ8iNmzUg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/Gen-AI-for-Data-Augmentation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0xsy-jYm22T",
        "outputId": "86edaabd-f04f-47e5-a061-ef6b12751b41"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D-o55hUmbpB",
        "outputId": "46596f34-af02-4d21-f324-f00586ca0130"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up device\n",
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DJfUHw5mbpB"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD7ifruzEF-K",
        "outputId": "c3437683-1572-44d0-ac37-7a18209b98a5"
      },
      "outputs": [],
      "source": [
        "!pip install clean-fid\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install -q transformers datasets accelerate sentencepiece\n",
        "!pip install -q git+https://github.com/huggingface/transformers\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEnqqVV6mbpB"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "from src.data.dataset import PetDatasetHandler\n",
        "from src.captioning.caption_generator import CaptionGenerator\n",
        "from src.captioning.captioning_blip_2 import BLIP2CaptionGenerator\n",
        "from src.captioning.git_caption_generator import GITCaptionGenerator\n",
        "from src.data.data_with_captions import PetDatasetWithCaptions\n",
        "from src.generation.text_generation import TextVariationGenerator\n",
        "from src.utils.logging import GANLogger\n",
        "from src.generation.image_generator import GANConfig, ConditionalGAN\n",
        "from src.training.callbacks import EarlyStopping, ModelCheckpoint, MetricsHistory\n",
        "from src.evaluation.metrics import FIDScore, CLIPScore, MetricsTracker\n",
        "from src.training.training import GANTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjRH_3IhmbpB"
      },
      "source": [
        "## Initialize and load dataset without transforms for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1J3f0djrmbpC"
      },
      "outputs": [],
      "source": [
        "data_dir = Path('./data')\n",
        "handler = PetDatasetHandler(data_dir)\n",
        "train_dataset, test_dataset = handler.load_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqincMqImbpC"
      },
      "source": [
        "### Basic dataset information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6H-4zr31mbpC",
        "outputId": "5cfa0881-f3ed-4533-b1f0-3075195783b7"
      },
      "outputs": [],
      "source": [
        "info = handler.get_dataset_info()\n",
        "print(\"Dataset Information:\")\n",
        "for key, value in info.items():\n",
        "    print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c4L9HRbmbpC"
      },
      "source": [
        "### Plot distributions and samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "h3-WAPGXmbpC",
        "outputId": "c7ef0f99-d3f1-49cf-d3c0-8d3aa1170f3d"
      },
      "outputs": [],
      "source": [
        "handler.plot_class_distribution().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "tyIYd0_SmbpC",
        "outputId": "375e165a-4979-47a7-e058-15a2b638669e"
      },
      "outputs": [],
      "source": [
        "handler.visualize_samples(9).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZTMNAxHmbpC"
      },
      "source": [
        "### Get detailed image statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yH1C4c-0mbpC",
        "outputId": "ab263331-d840-44f9-ae6f-72c3159e4c34"
      },
      "outputs": [],
      "source": [
        "stats = handler.get_image_stats(sample_size=100)\n",
        "print(\"\\nImage Statistics:\")\n",
        "for category, values in stats.items():\n",
        "    print(f\"\\n{category.upper()}:\")\n",
        "    for key, value in values.items():\n",
        "        print(f\"{key}: {value:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95ToX6RAmbpD"
      },
      "source": [
        "### For training, load with transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZP1GeQZYmbpD"
      },
      "outputs": [],
      "source": [
        "train_transforms = handler.get_training_transforms()\n",
        "train_dataset, test_dataset = handler.load_dataset(transform=train_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ha6Oe9S4hkK"
      },
      "source": [
        "## Image Captioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare caption generators: \n",
        "1. **Blip**\n",
        "2. **Blip-2**\n",
        "3. **GIT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurations\n",
        "\n",
        "def get_random_images(image_dir, count=5):\n",
        "    \"\"\"Randomly select images from the dataset.\"\"\"\n",
        "    image_paths = list(Path(image_dir).glob(\"*.jpg\"))\n",
        "    return random.sample(image_paths, min(count, len(image_paths)))\n",
        "\n",
        "data_dir = Path('./data')\n",
        "handler = PetDatasetHandler(data_dir)\n",
        "train_dataset, test_dataset = handler.load_dataset()\n",
        "\n",
        "image_dir = \"data/oxford-iiit-pet/images\"\n",
        "sample_images = get_random_images(image_dir, count=10)\n",
        "print(f\"Selected {len(sample_images)} random images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test BLIP original model\n",
        "\n",
        "print(\"Test di BLIP...\")\n",
        "blip_model = CaptionGenerator()\n",
        "blip_captions = {}\n",
        "\n",
        "for img_path in sample_images:\n",
        "    caption = blip_model.generate_caption(str(img_path))\n",
        "    blip_captions[str(img_path)] = caption\n",
        "    print(f\"BLIP - {img_path.name}: {caption}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test BLIP-2 model \n",
        "# Be Carefull !! Blip-2 is high consuming and high memory requiring, run this cell if you have high computational resources.\n",
        "\n",
        "print(\"\\nTest di BLIP-2...\")\n",
        "blip2_model = BLIP2CaptionGenerator(model_name=\"Salesforce/blip2-opt-2.7b\")\n",
        "blip2_captions = {}\n",
        "\n",
        "for img_path in sample_images:\n",
        "    caption = blip2_model.generate_caption(str(img_path))\n",
        "    blip2_captions[str(img_path)] = caption\n",
        "    print(f\"BLIP-2 - {img_path.name}: {caption}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test GIT model\n",
        "\n",
        "print(\"\\nTest di GIT...\")\n",
        "git_model_name = \"microsoft/git-base-coco\"\n",
        "processor_git = AutoProcessor.from_pretrained(git_model_name)\n",
        "model_git = AutoModelForCausalLM.from_pretrained(git_model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_git = model_git.to(device)\n",
        "\n",
        "git_captions = {}\n",
        "\n",
        "for img_path in sample_images:\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "    inputs_git = processor_git(images=image, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        generated_ids = model_git.generate(\n",
        "            pixel_values=inputs_git.pixel_values,\n",
        "            max_length=50,\n",
        "            num_beams=5\n",
        "        )\n",
        "    \n",
        "    caption = processor_git.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    git_captions[str(img_path)] = caption\n",
        "    print(f\"GIT - {img_path.name}: {caption}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize images with captions from three models (side by side)\n",
        "\n",
        "from textwrap import wrap\n",
        "\n",
        "def visualize_comparison(image_paths, blip_captions, blip2_captions, git_captions, wrap_width=30):\n",
        "    \"\"\"Visualizza il confronto tra le caption generate dai diversi modelli.\"\"\"\n",
        "    n_images = len(image_paths)\n",
        "    \n",
        "    fig, axes = plt.subplots(n_images, 3, figsize=(15, 5 * n_images))\n",
        "    \n",
        "    if n_images > 0:\n",
        "        axes[0, 0].set_title(\"BLIP\", fontsize=14)\n",
        "        axes[0, 1].set_title(\"BLIP-2\", fontsize=14)\n",
        "        axes[0, 2].set_title(\"GIT\", fontsize=14)\n",
        "    \n",
        "    for idx, img_path in enumerate(image_paths):\n",
        "        img_path_str = str(img_path)\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        \n",
        "        # BLIP\n",
        "        axes[idx, 0].imshow(img)\n",
        "        axes[idx, 0].axis('off')\n",
        "        wrapped_caption = \"\\n\".join(wrap(blip_captions[img_path_str], wrap_width))\n",
        "        axes[idx][0].set_xlabel(wrapped_caption, fontsize = 12)\n",
        "\n",
        "        # BLIP-2\n",
        "        axes[idx][1].imshow(img)\n",
        "        axes[idx][1].axis('off')\n",
        "        wrapped_caption = \"\\n\".join(wrap(blip2_captions[img_path_str], wrap_width))\n",
        "        axes[idx][1].set_xlabel(wrapped_caption, fontsize=12)\n",
        "        \n",
        "        # GIT\n",
        "        axes[idx][2].imshow(img)\n",
        "        axes[idx][2].axis('off')\n",
        "        wrapped_caption = \"\\n\".join(wrap(git_captions[img_path_str], wrap_width))\n",
        "        axes[idx][2].set_xlabel(wrapped_caption, fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "visualize_comparison(sample_images, blip_captions, blip2_captions, git_captions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADVSzXMi4hkK"
      },
      "source": [
        "### Initialize caption generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI2hQG804hkK"
      },
      "outputs": [],
      "source": [
        "caption_gen = GITCaptionGenerator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Dataset (If you haven’t done it before)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = Path('./data')\n",
        "handler = PetDatasetHandler(data_dir)\n",
        "train_dataset, test_dataset = handler.load_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "615M0HNu4hkK"
      },
      "source": [
        "### Test single image caption generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uzvz7bLQ4hkL",
        "outputId": "fd6e472c-c753-4531-96fc-d9124db20eba"
      },
      "outputs": [],
      "source": [
        "sample = random.randint(0, len(train_dataset)-1)\n",
        "sample_image_path = Path(train_dataset._images[sample])\n",
        "label = train_dataset.classes[train_dataset[sample][1]]\n",
        "caption = caption_gen.generate_caption(sample_image_path, label, max_length = 50)\n",
        "print(f\"Sample caption: {caption}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "6xGBj8fN6sbt",
        "outputId": "877d3a6f-0012-48ec-f50f-06c8b8f8949d"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.title(f\"{label}\")\n",
        "fig = plt.imshow(train_dataset[sample][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-N0xVkY4hkL"
      },
      "source": [
        "### Process a batch of images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1acVgcv4hkL",
        "outputId": "0d72b62d-bfbc-4b90-f06e-c743355bf315"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "image_paths = [Path(img) for img in train_dataset._images[:10]]\n",
        "labels = [train_dataset.classes[train_dataset[i][1]] for i in range(10)]\n",
        "captions = caption_gen.process_batch(image_paths, labels, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx7KTRb7Fm4d"
      },
      "source": [
        "### Process train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8p7xWxRFs7l"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "image_paths = [Path(img) for img in train_dataset._images]\n",
        "labels = [train_dataset.classes[train_dataset[i][1]] for i in range(len(image_paths))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVymaIVQGJrc",
        "outputId": "32f3af2c-4ee7-43e9-c38c-73c70f306fcb"
      },
      "outputs": [],
      "source": [
        "captions = caption_gen.process_batch(image_paths, labels, batch_size=batch_size)\n",
        "save_dir = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions')\n",
        "caption_gen.save_captions(save_dir / 'captions_traindataset.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsewHEa84hkL"
      },
      "source": [
        "### Save & Load captions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqkoWAjQ4hkL"
      },
      "outputs": [],
      "source": [
        "save_dir = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions')\n",
        "save_dir.mkdir(parents=True, exist_ok=True)\n",
        "caption_gen.save_captions(save_dir / 'captions_traindataset.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVdAoWbg4hkL"
      },
      "outputs": [],
      "source": [
        "caption_gen.load_captions(save_dir / 'captions.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-7Q6Z3B4hkL"
      },
      "source": [
        "### Visualize results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 970
        },
        "id": "OgmEGDzT4hkL",
        "outputId": "a203537e-48da-4269-b333-de33a2dcf9cd"
      },
      "outputs": [],
      "source": [
        "caption_gen.visualize_captions(num_samples=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb-GAogK4hkM"
      },
      "source": [
        "### Print some statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOjJeugs4hkM",
        "outputId": "71a32e59-99b5-49fe-8b97-352a8d0c3a4a"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nTotal captions generated: {len(caption_gen.captions_cache)}\")\n",
        "print(\"\\nSample of generated captions:\")\n",
        "for path, caption in list(caption_gen.captions_cache.items())[:3]:\n",
        "    print(f\"\\nImage: {Path(path).name}\")\n",
        "    print(f\"Caption: {caption}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Generations with Flan-T5 to increase numbers of captions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Initialize Text Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generator = TextVariationGenerator(model_name=\"google/flan-t5-large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### How it works\n",
        "> Comparison of different types of prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = generator.test_prompt_types(\"A white dog sitting on a brown chair\", temperature=0.95, num_variations=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test Quality for Few-Shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "original_caption = \"c\"\n",
        "\n",
        "variations = generator.test_few_shot_quality(\n",
        "    original_caption,\n",
        "    num_variations=3,\n",
        "    temperature=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load Captions saved and Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "caption_file = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions/captions_git_train_dataset.json')\n",
        "\n",
        "with open(caption_file, 'r') as f:\n",
        "    captions = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Data\n",
        "\n",
        "data_dir = Path('./data')\n",
        "handler = PetDatasetHandler(data_dir)\n",
        "train_dataset, test_dataset = handler.load_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test the variations on random captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_captions = dict(random.sample(list(captions.items()), 3))\n",
        "\n",
        "for img_path, caption in sample_captions.items():\n",
        "    print(f\"\\Image: {Path(img_path).name}\")\n",
        "    print(f\"Original Caption: {caption}\")\n",
        "    \n",
        "    variations = generator.generate_variations(\n",
        "        caption,\n",
        "        num_variations=3,\n",
        "        temperature=1,\n",
        "        prompt_type=\"few-shot\"\n",
        "    )\n",
        "    \n",
        "    print(\"New Versions:\")\n",
        "    for i, var in enumerate(variations):\n",
        "        print(f\"{i+1}. {var}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display images with caption variations\n",
        "\n",
        "def visualize_caption_variations(image_path, caption, variations):\n",
        "    \"\"\"View an image with the original caption and variations.\"\"\"\n",
        "    plt.figure(figsize=(10, 12))\n",
        "    \n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    plt.subplot(1, 1, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    \n",
        "    title = f\"Original: {caption}\\n\\nVariations:\\n\"\n",
        "    for i, var in enumerate(variations):\n",
        "        title += f\"{i+1}. {var}\\n\"\n",
        "    \n",
        "    plt.title(title, fontsize=10, loc='left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "sample_img_path = list(sample_captions.keys())[0]\n",
        "visualize_caption_variations(\n",
        "    sample_img_path,\n",
        "    sample_captions[sample_img_path],\n",
        "    variations\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Process entire caption file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_file = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions/flan_t5_variations.json')\n",
        "\n",
        "print(\"Generation of variations for all captions...\")\n",
        "variations_dict = generator.process_caption_file(\n",
        "    caption_file=caption_file,\n",
        "    output_file=output_file,\n",
        "    variations_per_caption=3,\n",
        "    class_balancing=True,\n",
        "    target_per_class=150,\n",
        "    min_variations=1,\n",
        "    max_variations=5,\n",
        "    prompt_type=\"few-shot\",\n",
        "    temperature=1\n",
        ")\n",
        "\n",
        "print(f\"Generate variations for {len(variations_dict)} caption\")\n",
        "print(f\"File saved in: {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluate captions generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_file = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions/flan_t5_variations.json')\n",
        "\n",
        "with open(output_file, 'r') as f:\n",
        "    variations_data = json.load(f)\n",
        "\n",
        "variations_counts = {img_path: len(vars_list) for img_path, vars_list in variations_data.items()}\n",
        "\n",
        "# Statistics\n",
        "avg_variations = sum(variations_counts.values()) / len(variations_counts)\n",
        "max_variations = max(variations_counts.values())\n",
        "min_variations = min(variations_counts.values())\n",
        "total_variations = sum(variations_counts.values())\n",
        "\n",
        "print(f\"Statistics of variations:\")\n",
        "print(f\"Total original captions: {len(variations_counts)}\")\n",
        "print(f\"Total generated variations: {total_variations}\")\n",
        "print(f\"Mean variation per caption: {avg_variations:.2f}\")\n",
        "print(f\"Max variations per caption: {max_variations}\")\n",
        "print(f\"Min variations per caption: {min_variations}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the breed from the captions and calculate statistics by breed\n",
        "\n",
        "def extract_breed(caption):\n",
        "    import re\n",
        "    match = re.search(r\"This is an? ([^\\.]+)\\.\", caption)\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "breed_counts_original = {}\n",
        "for caption in captions.values():\n",
        "    breed = extract_breed(caption)\n",
        "    if breed:\n",
        "        breed_counts_original[breed] = breed_counts_original.get(breed, 0) + 1\n",
        "\n",
        "breed_counts_variations = breed_counts_original.copy()\n",
        "for var_list in variations_data.values():\n",
        "    for var in var_list:\n",
        "        breed = extract_breed(var)\n",
        "        if breed:\n",
        "            breed_counts_variations[breed] = breed_counts_variations.get(breed, 0) + 1\n",
        "\n",
        "breeds = sorted(breed_counts_original.keys())\n",
        "original_counts = [breed_counts_original[breed] for breed in breeds]\n",
        "total_counts = [breed_counts_variations[breed] for breed in breeds]\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "x = range(len(breeds))\n",
        "plt.bar(x, original_counts, width=0.4, align='edge', label='Original')\n",
        "plt.bar([i+0.4 for i in x], total_counts, width=0.4, align='edge', label='After Variations')\n",
        "plt.xticks([i+0.2 for i in x], breeds, rotation=90)\n",
        "plt.xlabel('Breed')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Dataset Augmentation by Breed')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Balanced Subset selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "balanced_output = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions/balanced_flan_t5_variations.json')\n",
        "output_file = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions/flan_t5_variations.json')\n",
        "caption_file = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions/captions_git_train_dataset.json')\n",
        "\n",
        "balanced_subset = generator.select_balanced_subset(\n",
        "    caption_file=caption_file,\n",
        "    variations_file=output_file,\n",
        "    output_file=balanced_output,\n",
        "    target_per_class=150\n",
        ")\n",
        "\n",
        "print(f\"Created balanced subset with {len(balanced_subset)} captions\")\n",
        "print(f\"File saved in: {balanced_output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "breed_counts_balanced = {}\n",
        "for caption in balanced_subset.values():\n",
        "    breed = extract_breed(caption)\n",
        "    if breed:\n",
        "        breed_counts_balanced[breed] = breed_counts_balanced.get(breed, 0) + 1\n",
        "\n",
        "breeds = sorted(breed_counts_balanced.keys())\n",
        "balanced_counts = [breed_counts_balanced.get(breed, 0) for breed in breeds]\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.bar(breeds, balanced_counts)\n",
        "plt.xlabel('Breed')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Balanced Subset by Breed')\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Image Generation Diffusion Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.generation.image_diffusion_generator import DiffusionModelManager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Initialize Diffusion class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "diffusion_manager = DiffusionModelManager(\n",
        "    output_dir=\"/content/drive/MyDrive/outputs_master_ProfAI\",\n",
        "    default_model=\"runwayml/stable-diffusion-v1-5\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Zero-Shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models_to_test = [\n",
        "    \"stabilityai/stable-diffusion-2-1-base\",\n",
        "    #\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    \"kandinsky-community/kandinsky-2-2-decoder\",\n",
        "    \"black-forest-labs/FLUX.1-dev\"\n",
        "]\n",
        "\n",
        "for model in models_to_test:\n",
        "    diffusion_manager.test_diffusion_model(\n",
        "        model_id=model,\n",
        "        prompt=\"A high-quality photo of a British Shorthair cat\",\n",
        "        num_images=2,\n",
        "        output_subdir=\"model_comparison\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "caption_file = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions/flan_t5_variations.json')\n",
        "with open(caption_file, 'r') as f:\n",
        "    captions_data = json.load(f)\n",
        "\n",
        "sample_captions = random.sample(list(captions_data.values()), 3)\n",
        "print(\"Caption selected for this test:\")\n",
        "for i, caption in enumerate(sample_captions):\n",
        "    print(f\"{i+1}. {caption}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for caption in sample_captions:\n",
        "    diffusion_manager.test_diffusion_model(models_to_test[0], caption)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### LoRA fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Images and Captions from original dataset in the correct format\n",
        "\n",
        "captions_file = \"/content/drive/MyDrive/outputs_master_ProfAI/captions/captions_git_train_dataset.json\"\n",
        "images_dir = \"/content/Gen-AI-for-Data-Augmentation/data/oxford-iiit-pet/images\"\n",
        "\n",
        "dataset_dir = diffusion_manager.prepare_dataset(\n",
        "    captions_file=captions_file,\n",
        "    images_dir=images_dir,\n",
        "    max_samples_per_breed=20,\n",
        "    min_samples_per_breed=10,\n",
        "    target_total_samples=37*20,\n",
        "    resolution=512\n",
        ")\n",
        "\n",
        "variations_file = \"/content/drive/MyDrive/outputs_master_ProfAI/captions/flan_t5_variations.json\"\n",
        "validation_prompts = diffusion_manager.select_validation_prompts_from_variations(\n",
        "    variations_file=variations_file,\n",
        "    num_prompts=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run LoRA\n",
        "\n",
        "lora_model_dir = diffusion_manager.run_lora_training(\n",
        "    dataset_dir=dataset_dir,\n",
        "    output_name=\"pet_breeds_lora\",\n",
        "    base_model=\"runwayml/stable-diffusion-v1-5\",\n",
        "    resolution=512,\n",
        "    max_train_steps=1500,\n",
        "    learning_rate=5e-5,\n",
        "    validation_prompts=validation_prompts,\n",
        "    rank=4\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Inference with LoRA fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the fine-tuned model\n",
        "\n",
        "lora_model_dir = Path(\"/content/drive/MyDrive/outputs_master_ProfAI/lora_models/pet_breeds_lora/\")\n",
        "\n",
        "diffusion_manager.load_lora_model(\n",
        "    base_model_id=\"runwayml/stable-diffusion-v1-5\",\n",
        "    lora_weights_path=str(lora_model_dir / \"pytorch_lora_weights.safetensors\")\n",
        ")\n",
        "\n",
        "# Generate images with custom prompts\n",
        "test_prompts = [\n",
        "    \"A high-quality photograph of a Maine Coon cat with long fur\",\n",
        "    \"A detailed image of a Beagle dog running in a park\",\n",
        "    \"A professional photo of a Persian cat with blue eyes\"\n",
        "]\n",
        "\n",
        "generated_images = diffusion_manager.generate_images(\n",
        "    prompts=test_prompts,\n",
        "    output_subdir=\"test_generation\",\n",
        "    num_images_per_prompt=2,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=40\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate images from variations\n",
        "\n",
        "variation_images = diffusion_manager.generate_from_variations(\n",
        "    variations_file=variations_file,\n",
        "    output_subdir=\"variation_generation\",\n",
        "    num_samples=20,\n",
        "    num_images_per_prompt=1,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=40\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model Evaluation\n",
        "\n",
        "As said bu Diffusers team [here](https://huggingface.co/docs/diffusers/v0.26.1/conceptual/evaluation), evaluation should be done qualitatively. They also suggest to try other metrics (CLIP Score or FID) to have a quantitative point of view."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lora_model_dir = Path(\"/content/drive/MyDrive/outputs_master_ProfAI/lora_model\")\n",
        "variations_file = Path(\"/content/drive/MyDrive/outputs_master_ProfAI/captions/flan_t5_variations.json\")\n",
        "\n",
        "diffusion_manager.load_lora_model(\n",
        "    base_model_id=\"runwayml/stable-diffusion-v1-5\",\n",
        "    lora_weights_path=str(lora_model_dir / \"pytorch_lora_weights.safetensors\")\n",
        ")\n",
        "\n",
        "variation_images = diffusion_manager.generate_from_variations(\n",
        "    variations_file=variations_file,\n",
        "    output_subdir=\"variation_generation\",\n",
        "    num_samples=20,\n",
        "    num_images_per_prompt=1,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=40\n",
        ")\n",
        "\n",
        "all_images = []\n",
        "all_prompts = []\n",
        "for prompt, images in variation_images.items():\n",
        "    all_images.extend(images)\n",
        "    all_prompts.extend([prompt] * len(images))\n",
        "\n",
        "clip_scores = diffusion_manager.evaluate_clip_score(\n",
        "    images=all_images,\n",
        "    prompts=all_prompts\n",
        ")\n",
        "\n",
        "print(f\"Average CLIP score: {clip_scores['mean_clip_score']}\")\n",
        "print(f\"Individual scores: {clip_scores['individual_scores']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Generate Balanced Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Load Dataset (if you haven't done it before)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = Path('./data')\n",
        "handler = PetDatasetHandler(data_dir)\n",
        "train_dataset, test_dataset = handler.load_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Generate and Save Zip file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the fine-tuned model\n",
        "\n",
        "lora_model_dir = Path(\"/content/drive/MyDrive/outputs_master_ProfAI/lora_models/pet_breeds_lora/\")\n",
        "\n",
        "diffusion_manager.load_lora_model(\n",
        "    base_model_id=\"runwayml/stable-diffusion-v1-5\",\n",
        "    lora_weights_path=str(lora_model_dir / \"pytorch_lora_weights.safetensors\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate balanced dataset\n",
        "# Reminder: it is usefull to save images on Goggle Drive in order to restart generation in case of \n",
        "\n",
        "balanced_dataset_dir = diffusion_manager.generate_balanced_dataset(\n",
        "    variations_file=\"/content/drive/MyDrive/outputs_master_ProfAI/captions/flan_t5_variations.json\",\n",
        "    original_dataset_dir=\"/content/Gen-AI-for-Data-Augmentation/data/oxford-iiit-pet/images\",\n",
        "    target_dir=\"/content/drive/MyDrive/outputs_master_ProfAI/generated_data/\",\n",
        "    target_samples_per_class=150,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=40,\n",
        "    zip_result=True,\n",
        "    train_dataset = train_dataset,\n",
        "    time_limit_hours = 2.0,\n",
        "    resume_from_breed = \"Pug\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find black images generated by the fine-tuned model\n",
        "\n",
        "images_dir = Path(\"/content/drive/MyDrive/outputs_master_ProfAI/generated_data/\")\n",
        "threshold = 10\n",
        "\n",
        "black_images = []\n",
        "image_files = list(image_dir.glob(\"*.jpg\")) + list(image_dir.glob(\"*.png\"))\n",
        "print(f\"Analysis of {len(image_files)} images...\")\n",
        "    \n",
        "for img_path in image_files:\n",
        "    try:\n",
        "        img = Image.open(img_path)\n",
        "        img_array = np.array(img)\n",
        "        mean_brightness = np.mean(img_array)\n",
        "        if mean_brightness < threshold:\n",
        "          black_images.append(str(img_path))\n",
        "          print(f\"Black image found: {img_path.name} (brightness: {mean_brightness:.2f})\")\n",
        "    except Exception as e:\n",
        "        print(f\"Elaboration Error for {img_path.name}: {e}\")\n",
        "    \n",
        "print(f\"Found {len(black_images)} black images over {len(image_files)} total images\")\n",
        "\n",
        "if black_images:\n",
        "    output_file = images_dir / \"black_images.txt\"\n",
        "    with open(output_file, \"w\") as f:\n",
        "        for img_path in black_images:\n",
        "            f.write(f\"{img_path}\\n\")\n",
        "    print(f\"List of black images saved in {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove black images and count by class\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "black_images = []\n",
        "if output_file and os.path.exists(output_file):\n",
        "    with open(output_file, \"r\") as f:\n",
        "        black_images = [line.strip() for line in f if line.strip()]\n",
        "    print(f\"Loaded {len(black_images)} balck images to remove\")\n",
        "    \n",
        "    removed_count = 0\n",
        "    for img_path in black_images:\n",
        "        img_path = Path(img_path)\n",
        "        if img_path.exists():\n",
        "            img_path.unlink()\n",
        "            txt_path = img_path.with_suffix(\".txt\")\n",
        "            if txt_path.exists():\n",
        "                txt_path.unlink()    \n",
        "            removed_count += 1\n",
        "    \n",
        "    print(f\"Removed {removed_count} black images and corresponding txt files\")\n",
        "    \n",
        "class_counts = Counter()\n",
        "for img_path in images_dir.glob(\"*.jpg\"):\n",
        "    filename = img_path.stem\n",
        "    \n",
        "    if \"_gen_\" in filename:\n",
        "        class_name = filename.split(\"_gen_\")[0].replace(\"_\", \" \")\n",
        "        class_counts[class_name] += 1\n",
        "\n",
        "sorted_counts = dict(sorted(class_counts.items()))\n",
        "print(\"\\Images count per class:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"{'Class':<30} | {'Images':<10}\")\n",
        "print(\"-\"*50)\n",
        "for class_name, count in sorted_counts.items():\n",
        "    print(f\"{class_name:<30} | {count:<10}\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Total: {sum(sorted_counts.values())} generated images\")\n",
        "\n",
        "# Save a report\n",
        "report_path = images_dir / \"class_statistics.txt\"\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(\"Images count per class::\\n\")\n",
        "    f.write(\"=\"*50 + \"\\n\")\n",
        "    f.write(f\"{'Class':<30} | {'Images':<10}\\n\")\n",
        "    f.write(\"-\"*50 + \"\\n\")\n",
        "    for class_name, count in sorted_counts.items():\n",
        "        f.write(f\"{class_name:<30} | {count:<10}\\n\")\n",
        "    f.write(\"=\"*50 + \"\\n\")\n",
        "    f.write(f\"Total: {sum(sorted_counts.values())} generated images\\n\")\n",
        "\n",
        "print(f\"\\nReport saved in {report_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import yaml\n",
        "import torch\n",
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import random_split, Subset, ConcatDataset, DataLoader\n",
        "\n",
        "from src.utils.logger_setup_classifier import get_logger\n",
        "from src.models.model_factory import create_model, validate_model_config, get_available_models\n",
        "from src.training.trainer import ModelTrainer\n",
        "from src.training.experiment import Experiment\n",
        "from src.training.callbacks_classifier import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from src.visualization.plot_results import scatter_plot_metrics, plot_confusion_matrix, plot_misclassified_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup logging\n",
        "\n",
        "logger = get_logger(ch_log_level=logging.INFO, fh_log_level=logging.DEBUG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load config\n",
        "\n",
        "with open('config/config.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "\n",
        "BATCH_SIZE = config['training']['batch_size']\n",
        "NUM_EPOCHS = config['training']['num_epochs']\n",
        "LEARNING_RATE = config['training']['learning_rate']\n",
        "NUM_CLASSES = config['dataset']['num_classes']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download generated images from Drive folder and set up directories\n",
        "\n",
        "data_dir = Path('./data')\n",
        "generated_data_dir = data_dir / \"generated_data\"\n",
        "drive_generated_dir = Path(\"/content/drive/MyDrive/outputs_master_ProfAI/generated_data\")\n",
        "\n",
        "if not generated_data_dir.exists():\n",
        "    os.makedirs(generated_data_dir, exist_ok=True)\n",
        "\n",
        "if drive_generated_dir.exists():\n",
        "    jpg_files = list(drive_generated_dir.glob(\"*.jpg\"))\n",
        "    print(f\"Download {len(jpg_files)} images from Drive...\")\n",
        "    \n",
        "    for img_path in jpg_files:\n",
        "        dest_path = generated_data_dir / img_path.name\n",
        "        if not dest_path.exists():\n",
        "            shutil.copy(img_path, dest_path)\n",
        "    \n",
        "    print(f\"Loaded images in {generated_data_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "\n",
        "handler = PetDatasetHandler(data_dir)\n",
        "original_train_dataset, test_dataset = handler.load_dataset(transform=None)\n",
        "\n",
        "CLASS_NAMES = original_train_dataset.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define transformations\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(config['preprocessing']['image']['size']),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=config['preprocessing']['image']['mean'],\n",
        "        std=config['preprocessing']['image']['std']\n",
        "    )\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize(config['preprocessing']['image']['size']),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=config['preprocessing']['image']['mean'],\n",
        "        std=config['preprocessing']['image']['std']\n",
        "    )\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# You need to choose which test you want to run:\n",
        "    # 1. Only original dataset with no augmentation techniques\n",
        "    # 2. Only original dataset with augmentation techniques\n",
        "    # 3. Original dataset concatenated with generated dataset \n",
        "\n",
        "only_original = True\n",
        "augmentation = False\n",
        "generated = False\n",
        "\n",
        "\n",
        "if augmentation:\n",
        "    train_transform = transforms.Compose([\n",
        "    transforms.Resize(config['preprocessing']['image']['size']),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=config['preprocessing']['image']['mean'],\n",
        "        std=config['preprocessing']['image']['std']\n",
        "    )\n",
        "])\n",
        "    \n",
        "class TransformDataset:\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.dataset[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "if only_original and not augmentation:\n",
        "    print(\"Using only original dataset without augmentation\")\n",
        "    \n",
        "    train_size = int(0.8 * len(original_train_dataset))\n",
        "    val_size = len(original_train_dataset) - train_size\n",
        "    \n",
        "    temp_train, temp_val = random_split(\n",
        "        original_train_dataset, \n",
        "        [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "    \n",
        "    train_ds = TransformDataset(Subset(\n",
        "                temp_train,\n",
        "                list(range(len(temp_train)))),\n",
        "                transform=train_transform\n",
        "            )\n",
        "    val_ds = TransformDataset(Subset(\n",
        "                temp_val,\n",
        "                list(range(len(temp_val)))),\n",
        "                transform=val_test_transform\n",
        "            )    \n",
        "elif only_original and augmentation:\n",
        "    print(\"Using only original dataset with augmentation\")\n",
        "        \n",
        "    train_size = int(0.8 * len(original_train_dataset))\n",
        "    val_size = len(original_train_dataset) - train_size\n",
        "    \n",
        "    temp_train, temp_val = random_split(\n",
        "        original_train_dataset, \n",
        "        [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "    \n",
        "    train_ds = TransformDataset(Subset(\n",
        "                temp_train,\n",
        "                list(range(len(temp_train)))),\n",
        "                transform=train_transform\n",
        "            )\n",
        "    val_ds = TransformDataset(Subset(\n",
        "                temp_val,\n",
        "                list(range(len(temp_val)))),\n",
        "                transform=val_test_transform\n",
        "            )   \n",
        "else:\n",
        "    print(\"Using original dataset + generated images\")\n",
        "    \n",
        "    if not (generated_data_dir / \"organized\").exists():\n",
        "        organized_dir = generated_data_dir / \"organized\"\n",
        "        os.makedirs(organized_dir, exist_ok=True)\n",
        "        \n",
        "        class_mapping = {}\n",
        "        for img_path in generated_data_dir.glob(\"*.jpg\"):\n",
        "            if \"_gen_\" in img_path.stem:\n",
        "                class_name = img_path.stem.split(\"_gen_\")[0]\n",
        "                class_mapping[img_path.name] = class_name\n",
        "        \n",
        "        for img_name, class_name in class_mapping.items():\n",
        "            class_dir = organized_dir / class_name\n",
        "            os.makedirs(class_dir, exist_ok=True)\n",
        "            \n",
        "            src_path = generated_data_dir / img_name\n",
        "            if src_path.exists():\n",
        "                shutil.copy(src_path, class_dir / img_name)\n",
        "        \n",
        "        print(f\"Generated images organized in: {organized_dir}\")\n",
        "        generated_dir = organized_dir\n",
        "    else:\n",
        "        generated_dir = generated_data_dir / \"organized\"\n",
        "    \n",
        "    generated_dataset = ImageFolder(root=str(generated_dir), transform=None)\n",
        "    full_dataset = ConcatDataset([original_train_dataset, generated_dataset])\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    val_size = len(full_dataset) - train_size\n",
        "    temp_train, temp_val = random_split(\n",
        "        full_dataset, \n",
        "        [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "    train_ds = TransformDataset(Subset(\n",
        "                temp_train,\n",
        "                list(range(len(temp_train)))),\n",
        "                transform=train_transform\n",
        "            )\n",
        "    val_ds = TransformDataset(Subset(\n",
        "                temp_val,\n",
        "                list(range(len(temp_val)))),\n",
        "                transform=val_test_transform\n",
        "            )   \n",
        "\n",
        "# Transformation for test dataset\n",
        "test_ds = TransformDataset(Subset(\n",
        "                test_dataset,\n",
        "                list(range(len(test_dataset)))),\n",
        "                transform=val_test_transform\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    #num_workers=4,\n",
        "    #pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    #num_workers=4,\n",
        "    #pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    #num_workers=4,\n",
        "    #pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Train samples: {len(train_ds)}\")\n",
        "print(f\"Validation samples: {len(val_ds)}\")\n",
        "print(f\"Test samples: {len(test_ds)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count labels percentage for a chosen dataloader\n",
        "\n",
        "dataloader = train_loader\n",
        "\n",
        "class_counts = collections.defaultdict(int)\n",
        "for batch in dataloader:\n",
        "  _, labels = batch     \n",
        "  for label in labels:   \n",
        "    class_counts[label.item()] += 1  \n",
        "\n",
        "class_counts_dict = dict(sorted(class_counts.items()))\n",
        "class_names = CLASS_NAMES\n",
        "  \n",
        "print(f\"\\Class Distribution in Original Dataset - Train:\")\n",
        "print(f\"Total images: {len(dataloader.dataset)}\")\n",
        "print(\"-\" * 50)\n",
        "for class_idx, count in class_counts.items():\n",
        "    class_name = class_names[class_idx] if class_names else f\"Class {class_idx}\"\n",
        "    print(f\"{class_name:<30} | {count:>5} | {count/len(dataloader.dataset)*100:>6.2f}%\")\n",
        "\n",
        "print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration --> Baseline model\n",
        "\n",
        "model_config = {\n",
        "    'type': 'baseline',\n",
        "    'num_classes': 37,\n",
        "    'input_channels': 3\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration --> Transfer learning model (YOU NEED TO DECIDE IF TO USE CUSTOM CLASSIFIER OR NOT)\n",
        "\n",
        "model_config = {\n",
        "    'type': 'transfer',\n",
        "    'model_name': 'resnet50', \n",
        "    'num_classes': NUM_CLASSES,\n",
        "    'pretrained': True,\n",
        "    'use_custom_classifier': True\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Give a look at every avialable model\n",
        "\n",
        "get_available_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate configuration\n",
        "\n",
        "validate_model_config(model_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model\n",
        "\n",
        "model = create_model(model_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimizer e Loss\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup experiment\n",
        "\n",
        "experiment = Experiment(\n",
        "    name=\"resnet50_only_original_data\",\n",
        "    root=\"/content/drive/MyDrive/outputs_master_ProfAI/experiments_genAI\",\n",
        "    logger=logger\n",
        ")\n",
        "experiment.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup callbacks\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=config['training']['early_stopping']['patience'],\n",
        "        min_delta=config['training']['early_stopping']['min_delta'],\n",
        "        verbose=True\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        filepath='best_baseline_model.pth',\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        optimizer=optimizer,\n",
        "        mode='min',\n",
        "        patience=5,\n",
        "        factor=0.1,\n",
        "        verbose=True\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize trainer\n",
        "\n",
        "trainer = ModelTrainer(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    experiment=experiment,\n",
        "    device=device,\n",
        "    logger=logger\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "\n",
        "trained_model = trainer.train(\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "torch.save(trained_model.state_dict(), experiment.root / 'final_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation on test set\n",
        "\n",
        "test_logs = trainer.validate(test_loader)\n",
        "experiment.save_history('test', **test_logs)\n",
        "logger.info(f\"Test Results: {test_logs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get predictions on test set\n",
        "\n",
        "test_targets, test_predictions = trainer.predict(test_loader)\n",
        "\n",
        "plot_confusion_matrix(test_targets, test_predictions, classes = CLASS_NAMES, path_to_save=str(experiment.root))\n",
        "logger.info(\"Confusion matrix saved as 'confusion_matrix.png'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save test results\n",
        "test_results = {\n",
        "    'targets': test_targets.tolist(),\n",
        "    'predictions': test_predictions.tolist()\n",
        "}\n",
        "\n",
        "with open(f\"{experiment.results_dir}/test_results.json\", 'w') as f:\n",
        "    json.dump(test_results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate and Save plots training history\n",
        "\n",
        "experiment.plot_history()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation train and validation results\n",
        "\n",
        "scatter_plot_metrics(f'{experiment.root}/history/train.csv', \n",
        "                     f'{experiment.root}/history/val.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace missing values with 0 in column lr from val.csv and test.csv (TO BE REFACTORED)\n",
        "\n",
        "val = pd.read_csv(f\"{experiment.root}/history/val.csv\")\n",
        "val['lr'] = val['lr'].fillna(0).to_numpy()\n",
        "val.to_csv(f\"{experiment.root}/history/val.csv\", index=False)\n",
        "\n",
        "test = pd.read_csv(f\"{experiment.root}/history/test.csv\")\n",
        "test['lr'] = test['lr'].fillna(0).to_numpy()\n",
        "test.to_csv(f\"{experiment.root}/history/test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate average metrics for last n epochs\n",
        "\n",
        "experiment = Experiment(\"resnet34_only_original_data\", \"/content/drive/MyDrive/outputs_master_ProfAI/experiments_genAI\")\n",
        "experiment.load_history_from_file(\"val\")\n",
        "experiment.load_history_from_file(\"train\")\n",
        "experiment.load_history_from_file(\"test\")\n",
        "\n",
        "avg_metrics = experiment.calculate_average_metrics('val', last_n_epochs=5)\n",
        "print(\"Average validation metrics:\", avg_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export results in JSON\n",
        "\n",
        "experiment.export_results_to_json(\"/content/drive/MyDrive/outputs_master_ProfAI/experiments_genAI/resnet34_only_original_data/results/results.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find best epoch according to validation accuracy \n",
        "\n",
        "metric = 'accuracy'\n",
        "\n",
        "best_epoch = experiment.get_best_epoch(metric, mode='max')\n",
        "print(f\"Best validation accuracy was achieved at epoch {best_epoch} with\n",
        "       {100*experiment.history['val'][metric][best_epoch-1]:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot learning rate\n",
        "\n",
        "experiment.plot_learning_rate(experiment.history['train']['lr'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot misclassified images with ground truth and prediction\n",
        "\n",
        "plot_misclassified_images(\n",
        "    model=trained_model,\n",
        "    dataloader=test_loader,\n",
        "    device=device,\n",
        "    num_images=16,\n",
        "    class_names=CLASS_NAMES,\n",
        "    mean=config['preprocessing']['image']['mean'],\n",
        "    std=config['preprocessing']['image']['std']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUUthrS4Ac3h"
      },
      "source": [
        "## Imgae Generation with Conditional GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oY6xFDKYAc3h",
        "outputId": "e247399b-4a59-4963-da0e-3a8002639797"
      },
      "outputs": [],
      "source": [
        "# Setup device\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device used: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WzBPGaG_Ac3i"
      },
      "outputs": [],
      "source": [
        "# Configurations\n",
        "\n",
        "batch_size = 32\n",
        "image_size = 128\n",
        "num_workers = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnPm2yGnAc3i",
        "outputId": "cc737556-9022-4c98-8f2e-81a1d8fb4a5c"
      },
      "outputs": [],
      "source": [
        "# Setup components\n",
        "\n",
        "output_dir = Path(\"/content/drive/MyDrive/outputs_master_ProfAI\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "checkpoint_dir = output_dir / \"checkpoints\"\n",
        "log_dir = output_dir / \"logs\"\n",
        "\n",
        "metrics = MetricsTracker([\n",
        "    FIDScore(device=device),\n",
        "    CLIPScore(device=device)\n",
        "])\n",
        "\n",
        "logger = GANLogger(\"conditional_gan\", log_dir=log_dir)\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='fid', patience=5),\n",
        "    ModelCheckpoint(filepath=checkpoint_dir / \"best_model.pt\", monitor='fid'),\n",
        "    MetricsHistory(log_dir=log_dir / \"metrics\")\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "I0yjVD0295yM"
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "\n",
        "data_dir = Path('./data')\n",
        "handler = PetDatasetHandler(data_dir)\n",
        "train_dataset, test_dataset = handler.load_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "r6mtOOHp95yN"
      },
      "outputs": [],
      "source": [
        "# Load Captions\n",
        "with open('output/captions/captions_traindataset.json', 'r') as f:\n",
        "    caption_dict = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "TRoTLwvp95yN"
      },
      "outputs": [],
      "source": [
        "train_images_paths = [str(img) for img in train_dataset._images]\n",
        "\n",
        "test_images_paths = [str(img) for img in test_dataset._images]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thgLwkV0Ac3j",
        "outputId": "09f6f851-4fe2-4e56-9030-b8d027e9ad8b"
      },
      "outputs": [],
      "source": [
        "# Initialize train and val loader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "full_train_ds = PetDatasetWithCaptions(\n",
        "    image_paths=train_images_paths,\n",
        "    caption_dict=caption_dict,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_size = int(0.9 * len(full_train_ds))\n",
        "val_size = len(full_train_ds) - train_size\n",
        "train_ds, val_ds = random_split(full_train_ds, [train_size, val_size],\n",
        "                                generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "test_ds = PetDatasetWithCaptions(\n",
        "    image_paths=test_images_paths,\n",
        "    caption_dict=caption_dict,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PId4e9AMAc3k",
        "outputId": "6a9eb72d-3518-4cf8-fb2e-29d4fb3a7977"
      },
      "outputs": [],
      "source": [
        "# Initialize GAN model\n",
        "\n",
        "config = GANConfig(\n",
        "    latent_dim = 100,\n",
        "    caption_dim = 768,\n",
        "    image_size = image_size,\n",
        "    num_channels = 3,\n",
        "    generator_features = 64\n",
        ")\n",
        "\n",
        "gan = ConditionalGAN(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "eJwz9DZ-Ac3k"
      },
      "outputs": [],
      "source": [
        "# Initialize trainer\n",
        "\n",
        "trainer = GANTrainer(\n",
        "    gan=gan,\n",
        "    train_dataloader=train_loader,\n",
        "    val_dataloader=val_loader,\n",
        "    metrics_tracker=metrics,\n",
        "    logger=logger,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlyZmIyNAc3k",
        "outputId": "04f48ba7-5b95-4f12-8986-48c8929877c2"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "\n",
        "trainer.train(\n",
        "    num_epochs=100,\n",
        "    eval_freq=1,\n",
        "    sample_freq=500,\n",
        "    sample_dir=Path(\"samples\")\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
