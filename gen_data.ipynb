{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjIw6aznmbpA"
      },
      "source": [
        "# Generative AI project about Data Augmentation\n",
        "> *This is the notebook for the ninth Profession AI project about Generative AI module*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKM3dG8tmbpB"
      },
      "source": [
        "## Setup & Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMOFpywHmjlA",
        "outputId": "bba60265-03fe-481e-afc3-9bbe62d411af"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Silvano315/Gen-AI-for-Data-Augmentation.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3uLuJ8iNmzUg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/Gen-AI-for-Data-Augmentation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0xsy-jYm22T",
        "outputId": "86edaabd-f04f-47e5-a061-ef6b12751b41"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D-o55hUmbpB",
        "outputId": "46596f34-af02-4d21-f324-f00586ca0130"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DJfUHw5mbpB"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD7ifruzEF-K",
        "outputId": "c3437683-1572-44d0-ac37-7a18209b98a5"
      },
      "outputs": [],
      "source": [
        "!pip install clean-fid\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install -q transformers datasets accelerate sentencepiece\n",
        "!pip install -q git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pEnqqVV6mbpB"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import json\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "from src.data.dataset import PetDatasetHandler\n",
        "from src.captioning.caption_generator import CaptionGenerator\n",
        "from src.captioning.captioning_blip_2 import BLIP2CaptionGenerator\n",
        "from src.captioning.git_caption_generator import GITCaptionGenerator\n",
        "from src.data.data_with_captions import PetDatasetWithCaptions\n",
        "from src.generation.text_generation import TextVariationGenerator\n",
        "from src.utils.logging import GANLogger\n",
        "from src.generation.image_generator import GANConfig, ConditionalGAN\n",
        "from src.training.callbacks import EarlyStopping, ModelCheckpoint, MetricsHistory\n",
        "from src.evaluation.metrics import FIDScore, CLIPScore, MetricsTracker\n",
        "from src.training.training import GANTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjRH_3IhmbpB"
      },
      "source": [
        "## Initialize and load dataset without transforms for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1J3f0djrmbpC"
      },
      "outputs": [],
      "source": [
        "data_dir = Path('./data')\n",
        "handler = PetDatasetHandler(data_dir)\n",
        "train_dataset, test_dataset = handler.load_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqincMqImbpC"
      },
      "source": [
        "### Basic dataset information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6H-4zr31mbpC",
        "outputId": "5cfa0881-f3ed-4533-b1f0-3075195783b7"
      },
      "outputs": [],
      "source": [
        "info = handler.get_dataset_info()\n",
        "print(\"Dataset Information:\")\n",
        "for key, value in info.items():\n",
        "    print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c4L9HRbmbpC"
      },
      "source": [
        "### Plot distributions and samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "h3-WAPGXmbpC",
        "outputId": "c7ef0f99-d3f1-49cf-d3c0-8d3aa1170f3d"
      },
      "outputs": [],
      "source": [
        "handler.plot_class_distribution().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "tyIYd0_SmbpC",
        "outputId": "375e165a-4979-47a7-e058-15a2b638669e"
      },
      "outputs": [],
      "source": [
        "handler.visualize_samples(9).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZTMNAxHmbpC"
      },
      "source": [
        "### Get detailed image statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yH1C4c-0mbpC",
        "outputId": "ab263331-d840-44f9-ae6f-72c3159e4c34"
      },
      "outputs": [],
      "source": [
        "stats = handler.get_image_stats(sample_size=100)\n",
        "print(\"\\nImage Statistics:\")\n",
        "for category, values in stats.items():\n",
        "    print(f\"\\n{category.upper()}:\")\n",
        "    for key, value in values.items():\n",
        "        print(f\"{key}: {value:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95ToX6RAmbpD"
      },
      "source": [
        "### For training, load with transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZP1GeQZYmbpD"
      },
      "outputs": [],
      "source": [
        "train_transforms = handler.get_training_transforms()\n",
        "train_dataset, test_dataset = handler.load_dataset(transform=train_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ha6Oe9S4hkK"
      },
      "source": [
        "## Image Captioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare caption generators: \n",
        "1. **Blip**\n",
        "2. **Blip-2**\n",
        "3. **GIT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurations\n",
        "\n",
        "def get_random_images(image_dir, count=5):\n",
        "    \"\"\"Randomly select images from the dataset.\"\"\"\n",
        "    image_paths = list(Path(image_dir).glob(\"*.jpg\"))\n",
        "    return random.sample(image_paths, min(count, len(image_paths)))\n",
        "\n",
        "data_dir = Path('./data')\n",
        "handler = PetDatasetHandler(data_dir)\n",
        "train_dataset, test_dataset = handler.load_dataset()\n",
        "\n",
        "image_dir = \"data/oxford-iiit-pet/images\"\n",
        "sample_images = get_random_images(image_dir, count=10)\n",
        "print(f\"Selected {len(sample_images)} random images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test BLIP original model\n",
        "\n",
        "print(\"Test di BLIP...\")\n",
        "blip_model = CaptionGenerator()\n",
        "blip_captions = {}\n",
        "\n",
        "for img_path in sample_images:\n",
        "    caption = blip_model.generate_caption(str(img_path))\n",
        "    blip_captions[str(img_path)] = caption\n",
        "    print(f\"BLIP - {img_path.name}: {caption}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test BLIP-2 model \n",
        "# Be Carefull !! Blip-2 is high consuming and high memory requiring, run this cell if you have high computational resources.\n",
        "\n",
        "print(\"\\nTest di BLIP-2...\")\n",
        "blip2_model = BLIP2CaptionGenerator(model_name=\"Salesforce/blip2-opt-2.7b\")\n",
        "blip2_captions = {}\n",
        "\n",
        "for img_path in sample_images:\n",
        "    caption = blip2_model.generate_caption(str(img_path))\n",
        "    blip2_captions[str(img_path)] = caption\n",
        "    print(f\"BLIP-2 - {img_path.name}: {caption}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test GIT model\n",
        "\n",
        "print(\"\\nTest di GIT...\")\n",
        "git_model_name = \"microsoft/git-base-coco\"\n",
        "processor_git = AutoProcessor.from_pretrained(git_model_name)\n",
        "model_git = AutoModelForCausalLM.from_pretrained(git_model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_git = model_git.to(device)\n",
        "\n",
        "git_captions = {}\n",
        "\n",
        "for img_path in sample_images:\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "    inputs_git = processor_git(images=image, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        generated_ids = model_git.generate(\n",
        "            pixel_values=inputs_git.pixel_values,\n",
        "            max_length=50,\n",
        "            num_beams=5\n",
        "        )\n",
        "    \n",
        "    caption = processor_git.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    git_captions[str(img_path)] = caption\n",
        "    print(f\"GIT - {img_path.name}: {caption}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize images with captions from three models (side by side)\n",
        "\n",
        "from textwrap import wrap\n",
        "\n",
        "def visualize_comparison(image_paths, blip_captions, blip2_captions, git_captions, wrap_width=30):\n",
        "    \"\"\"Visualizza il confronto tra le caption generate dai diversi modelli.\"\"\"\n",
        "    n_images = len(image_paths)\n",
        "    \n",
        "    fig, axes = plt.subplots(n_images, 3, figsize=(15, 5 * n_images))\n",
        "    \n",
        "    if n_images > 0:\n",
        "        axes[0, 0].set_title(\"BLIP\", fontsize=14)\n",
        "        axes[0, 1].set_title(\"BLIP-2\", fontsize=14)\n",
        "        axes[0, 2].set_title(\"GIT\", fontsize=14)\n",
        "    \n",
        "    for idx, img_path in enumerate(image_paths):\n",
        "        img_path_str = str(img_path)\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        \n",
        "        # BLIP\n",
        "        axes[idx, 0].imshow(img)\n",
        "        axes[idx, 0].axis('off')\n",
        "        wrapped_caption = \"\\n\".join(wrap(blip_captions[img_path_str], wrap_width))\n",
        "        axes[idx][0].set_xlabel(wrapped_caption, fontsize = 12)\n",
        "\n",
        "        # BLIP-2\n",
        "        axes[idx][1].imshow(img)\n",
        "        axes[idx][1].axis('off')\n",
        "        wrapped_caption = \"\\n\".join(wrap(blip2_captions[img_path_str], wrap_width))\n",
        "        axes[idx][1].set_xlabel(wrapped_caption, fontsize=12)\n",
        "        \n",
        "        # GIT\n",
        "        axes[idx][2].imshow(img)\n",
        "        axes[idx][2].axis('off')\n",
        "        wrapped_caption = \"\\n\".join(wrap(git_captions[img_path_str], wrap_width))\n",
        "        axes[idx][2].set_xlabel(wrapped_caption, fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "visualize_comparison(sample_images, blip_captions, blip2_captions, git_captions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADVSzXMi4hkK"
      },
      "source": [
        "### Initialize caption generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI2hQG804hkK"
      },
      "outputs": [],
      "source": [
        "caption_gen = GITCaptionGenerator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Dataset (If you havenâ€™t done it before)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = Path('./data')\n",
        "handler = PetDatasetHandler(data_dir)\n",
        "train_dataset, test_dataset = handler.load_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "615M0HNu4hkK"
      },
      "source": [
        "### Test single image caption generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uzvz7bLQ4hkL",
        "outputId": "fd6e472c-c753-4531-96fc-d9124db20eba"
      },
      "outputs": [],
      "source": [
        "sample = random.randint(0, len(train_dataset)-1)\n",
        "sample_image_path = Path(train_dataset._images[sample])\n",
        "label = train_dataset.classes[train_dataset[sample][1]]\n",
        "caption = caption_gen.generate_caption(sample_image_path, label, max_length = 50)\n",
        "print(f\"Sample caption: {caption}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "6xGBj8fN6sbt",
        "outputId": "877d3a6f-0012-48ec-f50f-06c8b8f8949d"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.title(f\"{label}\")\n",
        "fig = plt.imshow(train_dataset[sample][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-N0xVkY4hkL"
      },
      "source": [
        "### Process a batch of images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1acVgcv4hkL",
        "outputId": "0d72b62d-bfbc-4b90-f06e-c743355bf315"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "image_paths = [Path(img) for img in train_dataset._images[:10]]\n",
        "labels = [train_dataset.classes[train_dataset[i][1]] for i in range(10)]\n",
        "captions = caption_gen.process_batch(image_paths, labels, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx7KTRb7Fm4d"
      },
      "source": [
        "### Process train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8p7xWxRFs7l"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "image_paths = [Path(img) for img in test_dataset._images]\n",
        "labels = [test_dataset.classes[test_dataset[i][1]] for i in range(len(image_paths))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVymaIVQGJrc",
        "outputId": "32f3af2c-4ee7-43e9-c38c-73c70f306fcb"
      },
      "outputs": [],
      "source": [
        "captions = caption_gen.process_batch(image_paths, labels, batch_size=batch_size)\n",
        "\n",
        "caption_gen.save_captions(save_dir / 'captions_testdataset.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsewHEa84hkL"
      },
      "source": [
        "### Save & Load captions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqkoWAjQ4hkL"
      },
      "outputs": [],
      "source": [
        "save_dir = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions')\n",
        "save_dir.mkdir(parents=True, exist_ok=True)\n",
        "caption_gen.save_captions(save_dir / 'captions_testdataset.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVdAoWbg4hkL"
      },
      "outputs": [],
      "source": [
        "caption_gen.load_captions(save_dir / 'captions.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-7Q6Z3B4hkL"
      },
      "source": [
        "### Visualize results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 970
        },
        "id": "OgmEGDzT4hkL",
        "outputId": "a203537e-48da-4269-b333-de33a2dcf9cd"
      },
      "outputs": [],
      "source": [
        "caption_gen.visualize_captions(num_samples=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb-GAogK4hkM"
      },
      "source": [
        "### Print some statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOjJeugs4hkM",
        "outputId": "71a32e59-99b5-49fe-8b97-352a8d0c3a4a"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nTotal captions generated: {len(caption_gen.captions_cache)}\")\n",
        "print(\"\\nSample of generated captions:\")\n",
        "for path, caption in list(caption_gen.captions_cache.items())[:3]:\n",
        "    print(f\"\\nImage: {Path(path).name}\")\n",
        "    print(f\"Caption: {caption}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Generations with Flan-T5 to increase numbers of captions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Initialize Text Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generator = TextVariationGenerator(model_name=\"google/flan-t5-large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### How it works\n",
        "> Comparison of different types of prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = generator.test_prompt_types(\"A white dog sitting on a brown chair\", temperature=0.95, num_variations=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test Quality for Few-Shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "original_caption = \"A gray cat sitting on a window sill - This is a British Shorthair.\"\n",
        "\n",
        "variations = generator.test_few_shot_quality(\n",
        "    original_caption,\n",
        "    num_variations=3,\n",
        "    temperature=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load Captions saved and Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "caption_file = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions/captions_git_train_dataset.json')\n",
        "\n",
        "with open(caption_file, 'r') as f:\n",
        "    captions = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Data\n",
        "\n",
        "data_dir = Path('./data')\n",
        "handler = PetDatasetHandler(data_dir)\n",
        "train_dataset, test_dataset = handler.load_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test the variations on random captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_captions = dict(random.sample(list(captions.items()), 3))\n",
        "\n",
        "for img_path, caption in sample_captions.items():\n",
        "    print(f\"\\Image: {Path(img_path).name}\")\n",
        "    print(f\"Original Caption: {caption}\")\n",
        "    \n",
        "    variations = generator.generate_variations(\n",
        "        caption,\n",
        "        num_variations=3,\n",
        "        temperature=1,\n",
        "        prompt_type=\"few-shot\"\n",
        "    )\n",
        "    \n",
        "    print(\"New Versions:\")\n",
        "    for i, var in enumerate(variations):\n",
        "        print(f\"{i+1}. {var}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display images with caption variations\n",
        "\n",
        "def visualize_caption_variations(image_path, caption, variations):\n",
        "    \"\"\"View an image with the original caption and variations.\"\"\"\n",
        "    plt.figure(figsize=(10, 12))\n",
        "    \n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    plt.subplot(1, 1, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    \n",
        "    title = f\"Original: {caption}\\n\\nVariations:\\n\"\n",
        "    for i, var in enumerate(variations):\n",
        "        title += f\"{i+1}. {var}\\n\"\n",
        "    \n",
        "    plt.title(title, fontsize=10, loc='left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "sample_img_path = list(sample_captions.keys())[0]\n",
        "visualize_caption_variations(\n",
        "    sample_img_path,\n",
        "    sample_captions[sample_img_path],\n",
        "    variations\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Process entire caption file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_file = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions/flan_t5_variations.json')\n",
        "\n",
        "print(\"Generation of variations for all captions...\")\n",
        "variations_dict = generator.process_caption_file(\n",
        "    caption_file=caption_file,\n",
        "    output_file=output_file,\n",
        "    variations_per_caption=3,\n",
        "    class_balancing=True,\n",
        "    target_per_class=150,\n",
        "    min_variations=1,\n",
        "    max_variations=5,\n",
        "    prompt_type=\"few-shot\",\n",
        "    temperature=1\n",
        ")\n",
        "\n",
        "print(f\"Generate variations for {len(variations_dict)} caption\")\n",
        "print(f\"File saved in: {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluate captions generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_file = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions/flan_t5_variations.json')\n",
        "\n",
        "with open(output_file, 'r') as f:\n",
        "    variations_data = json.load(f)\n",
        "\n",
        "variations_counts = {img_path: len(vars_list) for img_path, vars_list in variations_data.items()}\n",
        "\n",
        "# Statistics\n",
        "avg_variations = sum(variations_counts.values()) / len(variations_counts)\n",
        "max_variations = max(variations_counts.values())\n",
        "min_variations = min(variations_counts.values())\n",
        "total_variations = sum(variations_counts.values())\n",
        "\n",
        "print(f\"Statistics of variations:\")\n",
        "print(f\"Total original captions: {len(variations_counts)}\")\n",
        "print(f\"Total generated variations: {total_variations}\")\n",
        "print(f\"Mean variation per caption: {avg_variations:.2f}\")\n",
        "print(f\"Max variations per caption: {max_variations}\")\n",
        "print(f\"Min variations per caption: {min_variations}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the breed from the captions and calculate statistics by breed\n",
        "\n",
        "def extract_breed(caption):\n",
        "    import re\n",
        "    match = re.search(r\"This is an? ([^\\.]+)\\.\", caption)\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "breed_counts_original = {}\n",
        "for caption in captions.values():\n",
        "    breed = extract_breed(caption)\n",
        "    if breed:\n",
        "        breed_counts_original[breed] = breed_counts_original.get(breed, 0) + 1\n",
        "\n",
        "breed_counts_variations = breed_counts_original.copy()\n",
        "for var_list in variations_data.values():\n",
        "    for var in var_list:\n",
        "        breed = extract_breed(var)\n",
        "        if breed:\n",
        "            breed_counts_variations[breed] = breed_counts_variations.get(breed, 0) + 1\n",
        "\n",
        "breeds = sorted(breed_counts_original.keys())\n",
        "original_counts = [breed_counts_original[breed] for breed in breeds]\n",
        "total_counts = [breed_counts_variations[breed] for breed in breeds]\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "x = range(len(breeds))\n",
        "plt.bar(x, original_counts, width=0.4, align='edge', label='Original')\n",
        "plt.bar([i+0.4 for i in x], total_counts, width=0.4, align='edge', label='After Variations')\n",
        "plt.xticks([i+0.2 for i in x], breeds, rotation=90)\n",
        "plt.xlabel('Breed')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Dataset Augmentation by Breed')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Balanced Subset selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "balanced_output = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions/balanced_flan_t5_variations.json')\n",
        "output_file = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions/flan_t5_variations.json')\n",
        "\n",
        "balanced_subset = generator.select_balanced_subset(\n",
        "    caption_file=caption_file,\n",
        "    variations_file=output_file,\n",
        "    output_file=balanced_output,\n",
        "    target_per_class=150\n",
        ")\n",
        "\n",
        "print(f\"Created balanced subset with {len(balanced_subset)} captions\")\n",
        "print(f\"File saved in: {balanced_output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "breed_counts_balanced = {}\n",
        "for caption in balanced_subset.values():\n",
        "    breed = extract_breed(caption)\n",
        "    if breed:\n",
        "        breed_counts_balanced[breed] = breed_counts_balanced.get(breed, 0) + 1\n",
        "\n",
        "breeds = sorted(breed_counts_balanced.keys())\n",
        "balanced_counts = [breed_counts_balanced.get(breed, 0) for breed in breeds]\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.bar(breeds, balanced_counts)\n",
        "plt.xlabel('Breed')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Balanced Subset by Breed')\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Image Generation Diffusion Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zero-Shot Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "\n",
        "import torch\n",
        "from diffusers import StableDiffusionXLPipeline, StableDiffusionPipeline, DiffusionPipeline\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to load and test different models\n",
        "def test_diffusion_model(model_id, caption, num_images=1, seed=None):\n",
        "    \"\"\"\n",
        "    Test a diffusion model with a specific caption.\n",
        "    \n",
        "    Args:\n",
        "        model_id (str): Model ID to be tested\n",
        "        caption (str): Caption to use\n",
        "        num_images (int): Numbers of image to generate\n",
        "        seed (int, optional): Seed for the generation\n",
        "        \n",
        "    Returns:\n",
        "        list: List of generated images\n",
        "    \"\"\"\n",
        "    print(f\"Loading the model: {model_id}\")\n",
        "\n",
        "    if \"xl\" in model_id.lower():\n",
        "        pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "            model_id, \n",
        "            torch_dtype=torch.float16, \n",
        "            use_safetensors=True, \n",
        "            variant=\"fp16\"\n",
        "        )\n",
        "    elif \"kandinsky\" in model_id.lower():\n",
        "        pipe = DiffusionPipeline.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "    else:\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "    \n",
        "    pipe = pipe.to(\"cuda\")\n",
        "\n",
        "    # Save memory\n",
        "    pipe.enable_attention_slicing()\n",
        "    if hasattr(pipe, 'enable_vae_slicing'):\n",
        "        pipe.enable_vae_slicing()\n",
        "\n",
        "    breed_info = \"\"\n",
        "    if \" - This is a \" in caption:\n",
        "        parts = caption.split(\" - This is a \")\n",
        "        cleaned_caption = parts[0]\n",
        "        breed_info = parts[1].strip(\".\")\n",
        "        prompt = f\"A high-quality photo of a {breed_info}, {cleaned_caption}\"\n",
        "    else:\n",
        "        prompt = f\"A high-quality photo of {caption}\"\n",
        "\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    # Generazione delle immagini\n",
        "    images = []\n",
        "    for i in range(num_images):\n",
        "        # Set seed for reproducibility if provided\n",
        "        generator = None\n",
        "        if seed is not None:\n",
        "            generator = torch.Generator(device = \"cuda\").manual_seed(seed + i)\n",
        "\n",
        "        if \"kandinsky\" in model_id.lower():\n",
        "            image = pipe(prompt, guidance_scale=7.5).images[0]\n",
        "        else:\n",
        "            image = pipe(\n",
        "                prompt, \n",
        "                guidance_scale=7.5,\n",
        "                num_inference_steps=30,\n",
        "                generator=generator\n",
        "            ).images[0]\n",
        "        \n",
        "        images.append(image)\n",
        "\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(5*num_images, 5))\n",
        "    if num_images == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for i, img in enumerate(images):\n",
        "        axes[i].imshow(np.array(img))\n",
        "        axes[i].set_title(f\"Image {i+1}\")\n",
        "        axes[i].axis(\"off\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "caption_file = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions/balanced_flan_t5_variations.json')\n",
        "with open(caption_file, 'r') as f:\n",
        "    balanced_captions = json.load(f)\n",
        "\n",
        "sample_captions = random.sample(list(balanced_captions.values()), 3)\n",
        "print(\"Caption selected for this test:\")\n",
        "for i, caption in enumerate(sample_captions):\n",
        "    print(f\"{i+1}. {caption}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models_to_test = [\n",
        "    \"stabilityai/stable-diffusion-2-1-base\",\n",
        "    #\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    \"kandinsky-community/kandinsky-2-2-decoder\"\n",
        "]\n",
        "\n",
        "for caption in sample_captions:\n",
        "    test_diffusion_model(models_to_test[0], caption)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUUthrS4Ac3h"
      },
      "source": [
        "## Imgae Generation with Conditional GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oY6xFDKYAc3h",
        "outputId": "e247399b-4a59-4963-da0e-3a8002639797"
      },
      "outputs": [],
      "source": [
        "# Setup device\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device used: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WzBPGaG_Ac3i"
      },
      "outputs": [],
      "source": [
        "# Configurations\n",
        "\n",
        "batch_size = 32\n",
        "image_size = 128\n",
        "num_workers = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnPm2yGnAc3i",
        "outputId": "cc737556-9022-4c98-8f2e-81a1d8fb4a5c"
      },
      "outputs": [],
      "source": [
        "# Setup components\n",
        "\n",
        "output_dir = Path(\"/content/drive/MyDrive/outputs_master_ProfAI\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "checkpoint_dir = output_dir / \"checkpoints\"\n",
        "log_dir = output_dir / \"logs\"\n",
        "\n",
        "metrics = MetricsTracker([\n",
        "    FIDScore(device=device),\n",
        "    CLIPScore(device=device)\n",
        "])\n",
        "\n",
        "logger = GANLogger(\"conditional_gan\", log_dir=log_dir)\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='fid', patience=5),\n",
        "    ModelCheckpoint(filepath=checkpoint_dir / \"best_model.pt\", monitor='fid'),\n",
        "    MetricsHistory(log_dir=log_dir / \"metrics\")\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "I0yjVD0295yM"
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "\n",
        "data_dir = Path('./data')\n",
        "handler = PetDatasetHandler(data_dir)\n",
        "train_dataset, test_dataset = handler.load_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "r6mtOOHp95yN"
      },
      "outputs": [],
      "source": [
        "# Load Captions\n",
        "with open('output/captions/captions_traindataset.json', 'r') as f:\n",
        "    caption_dict = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "TRoTLwvp95yN"
      },
      "outputs": [],
      "source": [
        "train_images_paths = [str(img) for img in train_dataset._images]\n",
        "\n",
        "test_images_paths = [str(img) for img in test_dataset._images]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thgLwkV0Ac3j",
        "outputId": "09f6f851-4fe2-4e56-9030-b8d027e9ad8b"
      },
      "outputs": [],
      "source": [
        "# Initialize train and val loader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "full_train_ds = PetDatasetWithCaptions(\n",
        "    image_paths=train_images_paths,\n",
        "    caption_dict=caption_dict,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_size = int(0.9 * len(full_train_ds))\n",
        "val_size = len(full_train_ds) - train_size\n",
        "train_ds, val_ds = random_split(full_train_ds, [train_size, val_size],\n",
        "                                generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "test_ds = PetDatasetWithCaptions(\n",
        "    image_paths=test_images_paths,\n",
        "    caption_dict=caption_dict,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PId4e9AMAc3k",
        "outputId": "6a9eb72d-3518-4cf8-fb2e-29d4fb3a7977"
      },
      "outputs": [],
      "source": [
        "# Initialize GAN model\n",
        "\n",
        "config = GANConfig(\n",
        "    latent_dim = 100,\n",
        "    caption_dim = 768,\n",
        "    image_size = image_size,\n",
        "    num_channels = 3,\n",
        "    generator_features = 64\n",
        ")\n",
        "\n",
        "gan = ConditionalGAN(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "eJwz9DZ-Ac3k"
      },
      "outputs": [],
      "source": [
        "# Initialize trainer\n",
        "\n",
        "trainer = GANTrainer(\n",
        "    gan=gan,\n",
        "    train_dataloader=train_loader,\n",
        "    val_dataloader=val_loader,\n",
        "    metrics_tracker=metrics,\n",
        "    logger=logger,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlyZmIyNAc3k",
        "outputId": "04f48ba7-5b95-4f12-8986-48c8929877c2"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "\n",
        "trainer.train(\n",
        "    num_epochs=100,\n",
        "    eval_freq=1,\n",
        "    sample_freq=500,\n",
        "    sample_dir=Path(\"samples\")\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
