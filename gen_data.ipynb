{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjIw6aznmbpA"
      },
      "source": [
        "# Generative AI project about Data Augmentation\n",
        "> *This is the notebook for the ninth Profession AI project about Generative AI module*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKM3dG8tmbpB"
      },
      "source": [
        "## Setup & Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMOFpywHmjlA",
        "outputId": "bba60265-03fe-481e-afc3-9bbe62d411af"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Silvano315/Gen-AI-for-Data-Augmentation.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3uLuJ8iNmzUg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/Gen-AI-for-Data-Augmentation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0xsy-jYm22T",
        "outputId": "86edaabd-f04f-47e5-a061-ef6b12751b41"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D-o55hUmbpB",
        "outputId": "46596f34-af02-4d21-f324-f00586ca0130"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DJfUHw5mbpB"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD7ifruzEF-K",
        "outputId": "c3437683-1572-44d0-ac37-7a18209b98a5"
      },
      "outputs": [],
      "source": [
        "!pip install clean-fid\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install -q transformers datasets accelerate sentencepiece\n",
        "!pip install -q git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pEnqqVV6mbpB"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import json\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "from src.data.dataset import PetDatasetHandler\n",
        "from src.captioning.caption_generator import CaptionGenerator\n",
        "from src.captioning.captioning_blip_2 import BLIP2CaptionGenerator\n",
        "from src.captioning.git_caption_generator import GITCaptionGenerator\n",
        "from src.data.data_with_captions import PetDatasetWithCaptions\n",
        "from src.generation.text_generation import TextVariationGenerator\n",
        "from src.utils.logging import GANLogger\n",
        "from src.generation.image_generator import GANConfig, ConditionalGAN\n",
        "from src.training.callbacks import EarlyStopping, ModelCheckpoint, MetricsHistory\n",
        "from src.evaluation.metrics import FIDScore, CLIPScore, MetricsTracker\n",
        "from src.training.training import GANTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjRH_3IhmbpB"
      },
      "source": [
        "## Initialize and load dataset without transforms for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1J3f0djrmbpC"
      },
      "outputs": [],
      "source": [
        "data_dir = Path('./data')\n",
        "handler = PetDatasetHandler(data_dir)\n",
        "train_dataset, test_dataset = handler.load_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqincMqImbpC"
      },
      "source": [
        "### Basic dataset information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6H-4zr31mbpC",
        "outputId": "5cfa0881-f3ed-4533-b1f0-3075195783b7"
      },
      "outputs": [],
      "source": [
        "info = handler.get_dataset_info()\n",
        "print(\"Dataset Information:\")\n",
        "for key, value in info.items():\n",
        "    print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c4L9HRbmbpC"
      },
      "source": [
        "### Plot distributions and samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "h3-WAPGXmbpC",
        "outputId": "c7ef0f99-d3f1-49cf-d3c0-8d3aa1170f3d"
      },
      "outputs": [],
      "source": [
        "handler.plot_class_distribution().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "tyIYd0_SmbpC",
        "outputId": "375e165a-4979-47a7-e058-15a2b638669e"
      },
      "outputs": [],
      "source": [
        "handler.visualize_samples(9).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZTMNAxHmbpC"
      },
      "source": [
        "### Get detailed image statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yH1C4c-0mbpC",
        "outputId": "ab263331-d840-44f9-ae6f-72c3159e4c34"
      },
      "outputs": [],
      "source": [
        "stats = handler.get_image_stats(sample_size=100)\n",
        "print(\"\\nImage Statistics:\")\n",
        "for category, values in stats.items():\n",
        "    print(f\"\\n{category.upper()}:\")\n",
        "    for key, value in values.items():\n",
        "        print(f\"{key}: {value:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95ToX6RAmbpD"
      },
      "source": [
        "### For training, load with transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZP1GeQZYmbpD"
      },
      "outputs": [],
      "source": [
        "train_transforms = handler.get_training_transforms()\n",
        "train_dataset, test_dataset = handler.load_dataset(transform=train_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ha6Oe9S4hkK"
      },
      "source": [
        "## Image Captioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare caption generators: \n",
        "1. **Blip**\n",
        "2. **Blip-2**\n",
        "3. **GIT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurations\n",
        "\n",
        "def get_random_images(image_dir, count=5):\n",
        "    \"\"\"Randomly select images from the dataset.\"\"\"\n",
        "    image_paths = list(Path(image_dir).glob(\"*.jpg\"))\n",
        "    return random.sample(image_paths, min(count, len(image_paths)))\n",
        "\n",
        "data_dir = Path('./data')\n",
        "handler = PetDatasetHandler(data_dir)\n",
        "train_dataset, test_dataset = handler.load_dataset()\n",
        "\n",
        "image_dir = \"data/oxford-iiit-pet/images\"\n",
        "sample_images = get_random_images(image_dir, count=10)\n",
        "print(f\"Selected {len(sample_images)} random images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test BLIP original model\n",
        "\n",
        "print(\"Test di BLIP...\")\n",
        "blip_model = CaptionGenerator()\n",
        "blip_captions = {}\n",
        "\n",
        "for img_path in sample_images:\n",
        "    caption = blip_model.generate_caption(str(img_path))\n",
        "    blip_captions[str(img_path)] = caption\n",
        "    print(f\"BLIP - {img_path.name}: {caption}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test BLIP-2 model \n",
        "# Be Carefull !! Blip-2 is high consuming and high memory requiring, run this cell if you have high computational resources.\n",
        "\n",
        "print(\"\\nTest di BLIP-2...\")\n",
        "blip2_model = BLIP2CaptionGenerator(model_name=\"Salesforce/blip2-opt-2.7b\")\n",
        "blip2_captions = {}\n",
        "\n",
        "for img_path in sample_images:\n",
        "    caption = blip2_model.generate_caption(str(img_path))\n",
        "    blip2_captions[str(img_path)] = caption\n",
        "    print(f\"BLIP-2 - {img_path.name}: {caption}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test GIT model\n",
        "\n",
        "print(\"\\nTest di GIT...\")\n",
        "git_model_name = \"microsoft/git-base-coco\"\n",
        "processor_git = AutoProcessor.from_pretrained(git_model_name)\n",
        "model_git = AutoModelForCausalLM.from_pretrained(git_model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_git = model_git.to(device)\n",
        "\n",
        "git_captions = {}\n",
        "\n",
        "for img_path in sample_images:\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "    inputs_git = processor_git(images=image, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        generated_ids = model_git.generate(\n",
        "            pixel_values=inputs_git.pixel_values,\n",
        "            max_length=50,\n",
        "            num_beams=5\n",
        "        )\n",
        "    \n",
        "    caption = processor_git.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    git_captions[str(img_path)] = caption\n",
        "    print(f\"GIT - {img_path.name}: {caption}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize images with captions from three models (side by side)\n",
        "\n",
        "from textwrap import wrap\n",
        "\n",
        "def visualize_comparison(image_paths, blip_captions, blip2_captions, git_captions, wrap_width=30):\n",
        "    \"\"\"Visualizza il confronto tra le caption generate dai diversi modelli.\"\"\"\n",
        "    n_images = len(image_paths)\n",
        "    \n",
        "    fig, axes = plt.subplots(n_images, 3, figsize=(15, 5 * n_images))\n",
        "    \n",
        "    if n_images > 0:\n",
        "        axes[0, 0].set_title(\"BLIP\", fontsize=14)\n",
        "        axes[0, 1].set_title(\"BLIP-2\", fontsize=14)\n",
        "        axes[0, 2].set_title(\"GIT\", fontsize=14)\n",
        "    \n",
        "    for idx, img_path in enumerate(image_paths):\n",
        "        img_path_str = str(img_path)\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        \n",
        "        # BLIP\n",
        "        axes[idx, 0].imshow(img)\n",
        "        axes[idx, 0].axis('off')\n",
        "        wrapped_caption = \"\\n\".join(wrap(blip_captions[img_path_str], wrap_width))\n",
        "        axes[idx][0].set_xlabel(wrapped_caption, fontsize = 12)\n",
        "\n",
        "        # BLIP-2\n",
        "        axes[idx][1].imshow(img)\n",
        "        axes[idx][1].axis('off')\n",
        "        wrapped_caption = \"\\n\".join(wrap(blip2_captions[img_path_str], wrap_width))\n",
        "        axes[idx][1].set_xlabel(wrapped_caption, fontsize=12)\n",
        "        \n",
        "        # GIT\n",
        "        axes[idx][2].imshow(img)\n",
        "        axes[idx][2].axis('off')\n",
        "        wrapped_caption = \"\\n\".join(wrap(git_captions[img_path_str], wrap_width))\n",
        "        axes[idx][2].set_xlabel(wrapped_caption, fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "visualize_comparison(sample_images, blip_captions, blip2_captions, git_captions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADVSzXMi4hkK"
      },
      "source": [
        "### Initialize caption generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI2hQG804hkK"
      },
      "outputs": [],
      "source": [
        "caption_gen = GITCaptionGenerator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Dataset (If you haven’t done it before)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = Path('./data')\n",
        "handler = PetDatasetHandler(data_dir)\n",
        "train_dataset, test_dataset = handler.load_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "615M0HNu4hkK"
      },
      "source": [
        "### Test single image caption generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uzvz7bLQ4hkL",
        "outputId": "fd6e472c-c753-4531-96fc-d9124db20eba"
      },
      "outputs": [],
      "source": [
        "sample = random.randint(0, len(train_dataset)-1)\n",
        "sample_image_path = Path(train_dataset._images[sample])\n",
        "label = train_dataset.classes[train_dataset[sample][1]]\n",
        "caption = caption_gen.generate_caption(sample_image_path, label, max_length = 50)\n",
        "print(f\"Sample caption: {caption}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "6xGBj8fN6sbt",
        "outputId": "877d3a6f-0012-48ec-f50f-06c8b8f8949d"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.title(f\"{label}\")\n",
        "fig = plt.imshow(train_dataset[sample][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-N0xVkY4hkL"
      },
      "source": [
        "### Process a batch of images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1acVgcv4hkL",
        "outputId": "0d72b62d-bfbc-4b90-f06e-c743355bf315"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "image_paths = [Path(img) for img in train_dataset._images[:10]]\n",
        "labels = [train_dataset.classes[train_dataset[i][1]] for i in range(10)]\n",
        "captions = caption_gen.process_batch(image_paths, labels, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx7KTRb7Fm4d"
      },
      "source": [
        "### Process train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8p7xWxRFs7l"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "image_paths = [Path(img) for img in test_dataset._images]\n",
        "labels = [test_dataset.classes[test_dataset[i][1]] for i in range(len(image_paths))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVymaIVQGJrc",
        "outputId": "32f3af2c-4ee7-43e9-c38c-73c70f306fcb"
      },
      "outputs": [],
      "source": [
        "captions = caption_gen.process_batch(image_paths, labels, batch_size=batch_size)\n",
        "\n",
        "caption_gen.save_captions(save_dir / 'captions_testdataset.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsewHEa84hkL"
      },
      "source": [
        "### Save & Load captions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqkoWAjQ4hkL"
      },
      "outputs": [],
      "source": [
        "save_dir = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions')\n",
        "save_dir.mkdir(parents=True, exist_ok=True)\n",
        "caption_gen.save_captions(save_dir / 'captions_testdataset.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVdAoWbg4hkL"
      },
      "outputs": [],
      "source": [
        "caption_gen.load_captions(save_dir / 'captions.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-7Q6Z3B4hkL"
      },
      "source": [
        "### Visualize results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 970
        },
        "id": "OgmEGDzT4hkL",
        "outputId": "a203537e-48da-4269-b333-de33a2dcf9cd"
      },
      "outputs": [],
      "source": [
        "caption_gen.visualize_captions(num_samples=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb-GAogK4hkM"
      },
      "source": [
        "### Print some statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOjJeugs4hkM",
        "outputId": "71a32e59-99b5-49fe-8b97-352a8d0c3a4a"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nTotal captions generated: {len(caption_gen.captions_cache)}\")\n",
        "print(\"\\nSample of generated captions:\")\n",
        "for path, caption in list(caption_gen.captions_cache.items())[:3]:\n",
        "    print(f\"\\nImage: {Path(path).name}\")\n",
        "    print(f\"Caption: {caption}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Generations with Flan-T5 to increase numbers of captions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Initialize Text Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generator = TextVariationGenerator(model_name=\"google/flan-t5-large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### How it works\n",
        "> Comparison of different types of prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = generator.test_prompt_types(\"A white dog sitting on a brown chair\", temperature=0.95, num_variations=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test Quality for Few-Shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "original_caption = \"c\"\n",
        "\n",
        "variations = generator.test_few_shot_quality(\n",
        "    original_caption,\n",
        "    num_variations=3,\n",
        "    temperature=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load Captions saved and Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "caption_file = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions/captions_git_train_dataset.json')\n",
        "\n",
        "with open(caption_file, 'r') as f:\n",
        "    captions = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Data\n",
        "\n",
        "data_dir = Path('./data')\n",
        "handler = PetDatasetHandler(data_dir)\n",
        "train_dataset, test_dataset = handler.load_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test the variations on random captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_captions = dict(random.sample(list(captions.items()), 3))\n",
        "\n",
        "for img_path, caption in sample_captions.items():\n",
        "    print(f\"\\Image: {Path(img_path).name}\")\n",
        "    print(f\"Original Caption: {caption}\")\n",
        "    \n",
        "    variations = generator.generate_variations(\n",
        "        caption,\n",
        "        num_variations=3,\n",
        "        temperature=1,\n",
        "        prompt_type=\"few-shot\"\n",
        "    )\n",
        "    \n",
        "    print(\"New Versions:\")\n",
        "    for i, var in enumerate(variations):\n",
        "        print(f\"{i+1}. {var}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display images with caption variations\n",
        "\n",
        "def visualize_caption_variations(image_path, caption, variations):\n",
        "    \"\"\"View an image with the original caption and variations.\"\"\"\n",
        "    plt.figure(figsize=(10, 12))\n",
        "    \n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    plt.subplot(1, 1, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    \n",
        "    title = f\"Original: {caption}\\n\\nVariations:\\n\"\n",
        "    for i, var in enumerate(variations):\n",
        "        title += f\"{i+1}. {var}\\n\"\n",
        "    \n",
        "    plt.title(title, fontsize=10, loc='left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "sample_img_path = list(sample_captions.keys())[0]\n",
        "visualize_caption_variations(\n",
        "    sample_img_path,\n",
        "    sample_captions[sample_img_path],\n",
        "    variations\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Process entire caption file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_file = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions/flan_t5_variations.json')\n",
        "\n",
        "print(\"Generation of variations for all captions...\")\n",
        "variations_dict = generator.process_caption_file(\n",
        "    caption_file=caption_file,\n",
        "    output_file=output_file,\n",
        "    variations_per_caption=3,\n",
        "    class_balancing=True,\n",
        "    target_per_class=150,\n",
        "    min_variations=1,\n",
        "    max_variations=5,\n",
        "    prompt_type=\"few-shot\",\n",
        "    temperature=1\n",
        ")\n",
        "\n",
        "print(f\"Generate variations for {len(variations_dict)} caption\")\n",
        "print(f\"File saved in: {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluate captions generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_file = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions/flan_t5_variations.json')\n",
        "\n",
        "with open(output_file, 'r') as f:\n",
        "    variations_data = json.load(f)\n",
        "\n",
        "variations_counts = {img_path: len(vars_list) for img_path, vars_list in variations_data.items()}\n",
        "\n",
        "# Statistics\n",
        "avg_variations = sum(variations_counts.values()) / len(variations_counts)\n",
        "max_variations = max(variations_counts.values())\n",
        "min_variations = min(variations_counts.values())\n",
        "total_variations = sum(variations_counts.values())\n",
        "\n",
        "print(f\"Statistics of variations:\")\n",
        "print(f\"Total original captions: {len(variations_counts)}\")\n",
        "print(f\"Total generated variations: {total_variations}\")\n",
        "print(f\"Mean variation per caption: {avg_variations:.2f}\")\n",
        "print(f\"Max variations per caption: {max_variations}\")\n",
        "print(f\"Min variations per caption: {min_variations}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the breed from the captions and calculate statistics by breed\n",
        "\n",
        "def extract_breed(caption):\n",
        "    import re\n",
        "    match = re.search(r\"This is an? ([^\\.]+)\\.\", caption)\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "breed_counts_original = {}\n",
        "for caption in captions.values():\n",
        "    breed = extract_breed(caption)\n",
        "    if breed:\n",
        "        breed_counts_original[breed] = breed_counts_original.get(breed, 0) + 1\n",
        "\n",
        "breed_counts_variations = breed_counts_original.copy()\n",
        "for var_list in variations_data.values():\n",
        "    for var in var_list:\n",
        "        breed = extract_breed(var)\n",
        "        if breed:\n",
        "            breed_counts_variations[breed] = breed_counts_variations.get(breed, 0) + 1\n",
        "\n",
        "breeds = sorted(breed_counts_original.keys())\n",
        "original_counts = [breed_counts_original[breed] for breed in breeds]\n",
        "total_counts = [breed_counts_variations[breed] for breed in breeds]\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "x = range(len(breeds))\n",
        "plt.bar(x, original_counts, width=0.4, align='edge', label='Original')\n",
        "plt.bar([i+0.4 for i in x], total_counts, width=0.4, align='edge', label='After Variations')\n",
        "plt.xticks([i+0.2 for i in x], breeds, rotation=90)\n",
        "plt.xlabel('Breed')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Dataset Augmentation by Breed')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Balanced Subset selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "balanced_output = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions/balanced_flan_t5_variations.json')\n",
        "output_file = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions/flan_t5_variations.json')\n",
        "caption_file = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions/captions_git_train_dataset.json')\n",
        "\n",
        "balanced_subset = generator.select_balanced_subset(\n",
        "    caption_file=caption_file,\n",
        "    variations_file=output_file,\n",
        "    output_file=balanced_output,\n",
        "    target_per_class=150\n",
        ")\n",
        "\n",
        "print(f\"Created balanced subset with {len(balanced_subset)} captions\")\n",
        "print(f\"File saved in: {balanced_output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "breed_counts_balanced = {}\n",
        "for caption in balanced_subset.values():\n",
        "    breed = extract_breed(caption)\n",
        "    if breed:\n",
        "        breed_counts_balanced[breed] = breed_counts_balanced.get(breed, 0) + 1\n",
        "\n",
        "breeds = sorted(breed_counts_balanced.keys())\n",
        "balanced_counts = [breed_counts_balanced.get(breed, 0) for breed in breeds]\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.bar(breeds, balanced_counts)\n",
        "plt.xlabel('Breed')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Balanced Subset by Breed')\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Image Generation Diffusion Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zero-Shot Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "\n",
        "import torch\n",
        "from diffusers import StableDiffusionXLPipeline, StableDiffusionPipeline, AutoPipelineForText2Image, FluxPipeline\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to load and test different models\n",
        "def test_diffusion_model(model_id, caption, num_images=1, seed=None):\n",
        "    \"\"\"\n",
        "    Test a diffusion model with a specific caption.\n",
        "    \n",
        "    Args:\n",
        "        model_id (str): Model ID to be tested\n",
        "        caption (str): Caption to use\n",
        "        num_images (int): Numbers of image to generate\n",
        "        seed (int, optional): Seed for the generation\n",
        "        \n",
        "    Returns:\n",
        "        list: List of generated images\n",
        "    \"\"\"\n",
        "    print(f\"Loading the model: {model_id}\")\n",
        "\n",
        "    if \"xl\" in model_id.lower():\n",
        "        pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "            model_id, \n",
        "            torch_dtype=torch.float16, \n",
        "            use_safetensors=True, \n",
        "            variant=\"fp16\"\n",
        "        )\n",
        "    elif \"kandinsky\" in model_id.lower():\n",
        "        pipe = AutoPipelineForText2Image.from_pretrained(\n",
        "                    \"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16\n",
        "                  ).to(\"cuda\")\n",
        "    elif \"flux\" in model_id.lower():\n",
        "        pipe = FluxPipeline.from_pretrained(\n",
        "            \"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16 \n",
        "        )\n",
        "    else:\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "    \n",
        "    pipe = pipe.to(\"cuda\")\n",
        "\n",
        "    # Save memory\n",
        "    pipe.enable_attention_slicing()\n",
        "    if hasattr(pipe, 'enable_vae_slicing'):\n",
        "        pipe.enable_vae_slicing()\n",
        "\n",
        "    breed_info = \"\"\n",
        "    if \" - This is \" in caption[0]:\n",
        "        parts = caption[0].split(\" - This is a \")\n",
        "        cleaned_caption = parts[0]\n",
        "        breed_info = parts[1].strip(\".\")\n",
        "        prompt = f\"A high-quality photo of a {breed_info}, {cleaned_caption}\"\n",
        "    else:\n",
        "        prompt = f\"A high-quality photo of {caption}\"\n",
        "\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    images = []\n",
        "    for i in range(num_images):\n",
        "        generator = None\n",
        "        if seed is not None:\n",
        "            generator = torch.Generator(device = \"cuda\").manual_seed(seed + i)\n",
        "\n",
        "        if \"kandinsky\" in model_id.lower():\n",
        "            image = pipe(prompt, generator = generator).images[0]\n",
        "        elif \"flux\" in model_id.lower():\n",
        "            image = pipe(prompt,\n",
        "                          height=512,\n",
        "                          width=512,\n",
        "                          guidance_scale=3.5,\n",
        "                          num_inference_steps=50,\n",
        "                          max_sequence_length=512,\n",
        "                          generator=generator\n",
        "                        ).images[0]\n",
        "        else:\n",
        "            image = pipe(\n",
        "                prompt, \n",
        "                guidance_scale=7.5,\n",
        "                num_inference_steps=30,\n",
        "                generator=generator\n",
        "            ).images[0]\n",
        "        \n",
        "        images.append(image)\n",
        "\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(5*num_images, 5))\n",
        "    if num_images == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for i, img in enumerate(images):\n",
        "        axes[i].imshow(np.array(img))\n",
        "        axes[i].set_title(f\"Image {i+1}\")\n",
        "        axes[i].axis(\"off\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "caption_file = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions/balanced_flan_t5_variations.json')\n",
        "with open(caption_file, 'r') as f:\n",
        "    balanced_captions = json.load(f)\n",
        "\n",
        "sample_captions = random.sample(list(balanced_captions.values()), 3)\n",
        "print(\"Caption selected for this test:\")\n",
        "for i, caption in enumerate(sample_captions):\n",
        "    print(f\"{i+1}. {caption}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models_to_test = [\n",
        "    \"stabilityai/stable-diffusion-2-1-base\",\n",
        "    #\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    \"kandinsky-community/kandinsky-2-2-decoder\",\n",
        "    \"black-forest-labs/FLUX.1-dev\"\n",
        "]\n",
        "\n",
        "for caption in sample_captions:\n",
        "    test_diffusion_model(models_to_test[0], caption)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LoRA Fine Tuning\n",
        "> Give a look [here](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README.md)\n",
        "\n",
        "> This [link](https://huggingface.co/docs/diffusers/v0.13.0/en/training/lora) to understand LoRA and an example of it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diffusers installation\n",
        "\n",
        "!git clone https://github.com/huggingface/diffusers\n",
        "!cd diffusers\n",
        "!pip install /content/Gen-AI-for-Data-Augmentation/diffusers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Libraries \n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import subprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_dataset(\n",
        "    captions_file,\n",
        "    images_dir,\n",
        "    output_dir,\n",
        "    max_samples_per_breed=None,\n",
        "    min_samples_per_breed=3,\n",
        "    target_total_samples=100\n",
        "):\n",
        "    \"\"\"\n",
        "    Prepare dataset for LoRA Stable Diffusion fine-tuning.\n",
        "    \n",
        "    Args:\n",
        "        captions_file: Path to the JSON file with captions\n",
        "        images_dir: Directory containing the images\n",
        "        output_dir: Output directory for the prepared dataset\n",
        "        max_samples_per_breed: Maximum number of samples per breed\n",
        "        min_samples_per_breed: Minimum number of samples per breed\n",
        "        target_total_samples: Total target number of samples\n",
        "    \"\"\"\n",
        "\n",
        "    images_output_dir = Path(output_dir)\n",
        "    os.makedirs(images_output_dir, exist_ok=True)\n",
        "    \n",
        "    with open(captions_file, \"r\") as f:\n",
        "        captions = json.load(f)\n",
        "    \n",
        "    breed_samples = {}\n",
        "    for img_path, caption in captions.items():\n",
        "        if \" - This is a \" in caption:\n",
        "            breed = caption.split(\" - This is a \")[1].strip(\".\")\n",
        "            if breed not in breed_samples:\n",
        "                breed_samples[breed] = []\n",
        "            \n",
        "            img_name = Path(img_path).name\n",
        "            full_img_path = Path(images_dir) / img_name\n",
        "\n",
        "            if full_img_path.exists():\n",
        "                breed_samples[breed].append((str(full_img_path), caption))\n",
        "\n",
        "    \n",
        "    # Select a balanced subset\n",
        "    selected_samples = []\n",
        "    metadata = []\n",
        "    \n",
        "    for breed, samples in breed_samples.items():\n",
        "        num_samples = min(\n",
        "            len(samples),\n",
        "            max_samples_per_breed if max_samples_per_breed else len(samples)\n",
        "        )\n",
        "        num_samples = max(num_samples, min_samples_per_breed)\n",
        "        \n",
        "        # Select random sample\n",
        "        breed_selection = random.sample(samples, min(num_samples, len(samples)))\n",
        "        selected_samples.extend(breed_selection)\n",
        "    \n",
        "    # Limit with target_total_samples\n",
        "    if target_total_samples and len(selected_samples) > target_total_samples:\n",
        "        random.shuffle(selected_samples)\n",
        "        selected_samples = selected_samples[:target_total_samples]\n",
        "    \n",
        "    print(f\"Selected {len(selected_samples)} samples from {len(breed_samples)} breed\")\n",
        "    \n",
        "    # Create a metadata.jsonl as required by LoRA\n",
        "    for i, (img_path, caption) in enumerate(selected_samples):\n",
        "        dest_filename = f\"image_{i:06d}.jpg\"\n",
        "        dest_path = images_output_dir / dest_filename\n",
        "        shutil.copy(img_path, dest_path)\n",
        "        \n",
        "        metadata.append({\n",
        "            \"file_name\": f\"images/{dest_filename}\",\n",
        "            \"text\": caption\n",
        "        })\n",
        "    \n",
        "    with open(Path(output_dir) / \"metadata.jsonl\", \"w\") as f:\n",
        "        for item in metadata:\n",
        "            f.write(json.dumps(item) + \"\\n\")\n",
        "    \n",
        "    print(f\"Dataset prepared in {output_dir}\")\n",
        "    return output_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_lora_training(\n",
        "    dataset_dir,\n",
        "    output_dir,\n",
        "    base_model=\"runwayml/stable-diffusion-v1-5\",\n",
        "    resolution=512,\n",
        "    train_batch_size=1,\n",
        "    max_train_steps=1000,\n",
        "    validation_prompts=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Fine-tuning LoRA using the Hugging Face script.\n",
        "    \"\"\"\n",
        "    # Correct diffusers version if you didn't run it before\n",
        "    subprocess.run([\"pip\", \"install\", \"-q\", \"git+https://github.com/huggingface/diffusers.git\"])\n",
        "    \n",
        "    # We need to locally download the most up-to-date version of train_text_to_image_lora.py\n",
        "    subprocess.run([\"wget\", \"https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/train_text_to_image_lora.py\", \"-O\", \"train_text_to_image_lora.py\"])\n",
        "    \n",
        "    # TO-DO: improve validation_prompts if validation_prompts is None \n",
        "    if validation_prompts is None:\n",
        "        validation_prompts = [\n",
        "            \"A high-quality photo of a dog\",\n",
        "            \"A high-quality photo of a cat\",\n",
        "            \"A close-up portrait of a pet\"\n",
        "        ]\n",
        "    \n",
        "    # Command to use train_text_to_image_lora.py\n",
        "    cmd = [\n",
        "        \"accelerate\", \"launch\",\n",
        "        \"train_text_to_image_lora.py\",\n",
        "        f\"--pretrained_model_name_or_path={base_model}\",\n",
        "        f\"--train_data_dir={dataset_dir}\",\n",
        "        f\"--output_dir={output_dir}\",\n",
        "        f\"--resolution={resolution}\",\n",
        "        \"--center_crop\",\n",
        "        \"--random_flip\",\n",
        "        f\"--train_batch_size={train_batch_size}\",\n",
        "        \"--gradient_accumulation_steps=4\",\n",
        "        \"--gradient_checkpointing\",\n",
        "        \"--mixed_precision=fp16\",\n",
        "        f\"--max_train_steps={max_train_steps}\",\n",
        "        \"--learning_rate=1e-04\",\n",
        "        \"--lr_scheduler=constant\",\n",
        "        \"--lr_warmup_steps=0\",\n",
        "        \"--validation_epochs=100\",\n",
        "        f\"--validation_prompt=\\\"{'; '.join(validation_prompts)}\\\"\",\n",
        "        \"--seed=42\",\n",
        "        \"--checkpointing_steps=500\"\n",
        "    ]\n",
        "    \n",
        "    print(f\"Start of LoRA training...\")\n",
        "    print(f\"Command: {' '.join(cmd)}\")\n",
        "    process = subprocess.run(cmd, capture_output=True, text=True)\n",
        "    print(f\"Standard output: {process.stdout}\")\n",
        "    print(f\"Standard error: {process.stderr}\")\n",
        "    print(f\"Training completed. Results in {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuratios + Prepare dataset + Create validation prompts\n",
        "\n",
        "captions_file = \"/content/drive/MyDrive/outputs_master_ProfAI/captions/captions_git_train_dataset.json\"\n",
        "images_dir = \"/content/Gen-AI-for-Data-Augmentation/data/oxford-iiit-pet/images\"\n",
        "dataset_dir = \"/content/Gen-AI-for-Data-Augmentation/lora_dataset\"\n",
        "output_dir = \"/content/Gen-AI-for-Data-Augmentation/lora_model\"\n",
        "\n",
        "prepare_dataset(\n",
        "    captions_file,\n",
        "    images_dir,\n",
        "    dataset_dir,\n",
        "    max_samples_per_breed=10,\n",
        "    min_samples_per_breed=3,\n",
        "    target_total_samples=37*10\n",
        ")\n",
        "\n",
        "def select_validation_prompts_from_variations(variations_file, num_prompts=5, seed=42):\n",
        "    \"\"\"Select some validation prompts from the generated variations.\"\"\"\n",
        "    with open(variations_file, 'r') as f:\n",
        "        variations = json.load(f)\n",
        "    \n",
        "    all_captions = []\n",
        "    for variations_list in variations.values():\n",
        "        all_captions.extend(variations_list)\n",
        "    \n",
        "    # Random captions selection as validation prompts\n",
        "    random.seed(seed)\n",
        "    selected_prompts = random.sample(all_captions, min(num_prompts, len(all_captions)))\n",
        "    \n",
        "    return selected_prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start LoRA training\n",
        "\n",
        "variations_file = \"/content/drive/MyDrive/outputs_master_ProfAI/captions/flan_t5_variations.json\"\n",
        "validation_prompts = select_validation_prompts_from_variations(variations_file)\n",
        "\n",
        "run_lora_training(\n",
        "    dataset_dir,\n",
        "    output_dir,\n",
        "    base_model = \"runwayml/stable-diffusion-v1-5\",\n",
        "    resolution = 128,\n",
        "    train_batch_size = 1,\n",
        "    max_train_steps=800,\n",
        "    validation_prompts=validation_prompts\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "\n",
        "import torch\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
        "from typing import List, Union, Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_lora_model(\n",
        "    base_model_id: str = \"runwayml/stable-diffusion-v1-5\",\n",
        "    lora_weights_path: Optional[str] = None,\n",
        "    device: str = \"cuda\",\n",
        "    torch_dtype = torch.float16\n",
        "):\n",
        "    \"\"\"\n",
        "    Load Stable Diffusion model with LoRA weights.\n",
        "    \n",
        "    Args:\n",
        "        base_model_id (str): base Stable Diffusion model ID \n",
        "        lora_weights_path (str): Path to trained LoRA weights\n",
        "        device (str): Device to use (cuda o cpu)\n",
        "        torch_dtype: Precision to use (float16 o float32)\n",
        "    \n",
        "    Returns:\n",
        "        pipeline: Pipeline SD configured with LoRA\n",
        "    \"\"\"\n",
        "    print(f\"Loading base model {base_model_id}...\")\n",
        "    \n",
        "    pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "        base_model_id,\n",
        "        torch_dtype=torch_dtype\n",
        "    )\n",
        "    \n",
        "    # Optimize the pipeline\n",
        "    pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
        "    \n",
        "    if lora_weights_path:\n",
        "        print(f\"Loading LoRA weights from {lora_weights_path}...\")\n",
        "        pipeline.unet.load_attn_procs(lora_weights_path)\n",
        "        print(\"LoRA weights loaded with success!\")\n",
        "    \n",
        "    pipeline.to(device)\n",
        "    \n",
        "    # Optimize memory\n",
        "    pipeline.enable_attention_slicing()\n",
        "    \n",
        "    return pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_images(\n",
        "    pipeline,\n",
        "    prompts: Union[str, List[str]],\n",
        "    output_dir: Optional[str] = None,\n",
        "    num_images_per_prompt: int = 1,\n",
        "    guidance_scale: float = 7.5,\n",
        "    num_inference_steps: int = 30,\n",
        "    seed: Optional[int] = None,\n",
        "    width: int = 512,\n",
        "    height: int = 512,\n",
        "    save_images: bool = True,\n",
        "    display_images: bool = True,\n",
        "    batch_size: int = 1\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate images from the model with LoRA using specified prompts.\n",
        "    \n",
        "    Args:\n",
        "        pipeline: Stable Diffusion Pipeline\n",
        "        prompts: List of prompts or single prompt\n",
        "        output_dir: Directory where to save images\n",
        "        num_images_per_prompt: Number of images to be generated for each prompt\n",
        "        guidance_scale: Guidance factor (higher = more faithful to the text)\n",
        "        num_inference_steps: Number of inference steps\n",
        "        seed: Seed for the generation\n",
        "        width: Image width\n",
        "        height: Image height\n",
        "        save_images: Whether to save images on disk\n",
        "        display_images: Whether to display images\n",
        "        batch_size: Batch size for generation\n",
        "    \n",
        "    Returns:\n",
        "        Dict[str, List[Image.Image]]: Dictionary that maps prompts to generated images\n",
        "    \"\"\"\n",
        "    if isinstance(prompts, str):\n",
        "        prompts = [prompts]\n",
        "    \n",
        "    if save_images and output_dir:\n",
        "        output_path = Path(output_dir)\n",
        "        output_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    results = {}\n",
        "    all_images = []\n",
        "    all_prompts = []\n",
        "\n",
        "    for i in range(0, len(prompts), batch_size):\n",
        "        batch_prompts = prompts[i:i + batch_size]\n",
        "        \n",
        "        generator = None\n",
        "        if seed is not None:\n",
        "            generator = torch.Generator(device=pipeline.device).manual_seed(seed)\n",
        "            seed += 1\n",
        "        \n",
        "        print(f\"Batch Generation {i//batch_size + 1}/{(len(prompts)-1)//batch_size + 1}...\")\n",
        "        batch_results = pipeline(\n",
        "            batch_prompts,\n",
        "            num_images_per_prompt=num_images_per_prompt,\n",
        "            guidance_scale=guidance_scale,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            generator=generator,\n",
        "            width=width,\n",
        "            height=height\n",
        "        )\n",
        "\n",
        "        for j, prompt in enumerate(batch_prompts):\n",
        "            images = batch_results.images[j*num_images_per_prompt:(j+1)*num_images_per_prompt]\n",
        "            results[prompt] = images\n",
        "            all_images.extend(images)\n",
        "            all_prompts.extend([prompt] * num_images_per_prompt)\n",
        "            \n",
        "            if save_images and output_dir:\n",
        "                for k, img in enumerate(images):\n",
        "                    prompt_hash = abs(hash(prompt)) % 10000\n",
        "                    img_path = output_path / f\"gen_{prompt_hash}_{i}_{j}_{k}.png\"\n",
        "                    img.save(img_path)\n",
        "                    \n",
        "                    with open(output_path / f\"gen_{prompt_hash}_{i}_{j}_{k}.txt\", \"w\") as f:\n",
        "                        f.write(prompt)\n",
        "\n",
        "    if display_images:\n",
        "        n_cols = min(5, len(all_images))\n",
        "        n_rows = (len(all_images) + n_cols - 1) // n_cols\n",
        "        \n",
        "        plt.figure(figsize=(n_cols * 5, n_rows * 6))\n",
        "        \n",
        "        for i, (image, prompt) in enumerate(zip(all_images, all_prompts)):\n",
        "            plt.subplot(n_rows, n_cols, i + 1)\n",
        "            plt.imshow(np.array(image))\n",
        "            plt.title(prompt[:50] + \"...\" if len(prompt) > 50 else prompt, fontsize=8)\n",
        "            plt.axis(\"off\")\n",
        "            \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_from_variations(\n",
        "    pipeline,\n",
        "    variations_file: str,\n",
        "    output_dir: str,\n",
        "    num_samples: int = 5,\n",
        "    num_images_per_prompt: int = 1,\n",
        "    seed: Optional[int] = None,\n",
        "    random_variations: bool = True\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate images from the caption variations in the specified file.\n",
        "    \n",
        "    Args:\n",
        "        pipeline: Stable Diffusion Pipeline\n",
        "        variations_file: JSON file of variations\n",
        "        output_dir: Directory where to save images\n",
        "        num_samples: Number of variations to be used\n",
        "        num_images_per_prompt: Number of images per prompt\n",
        "        seed: Seed for the generation\n",
        "        random_variations: Whether to select random variations or first ones\n",
        "    \n",
        "    Returns:\n",
        "        Dict[str, List[Image.Image]]: Dictionary that maps prompts to generated images\n",
        "    \"\"\"\n",
        "    with open(variations_file, \"r\") as f:\n",
        "        variations_data = json.load(f)\n",
        "    \n",
        "    all_variations = []\n",
        "    for img_path, variations in variations_data.items():\n",
        "        all_variations.extend(variations)\n",
        "    \n",
        "    if random_variations:\n",
        "        selected_variations = random.sample(all_variations, min(num_samples, len(all_variations)))\n",
        "    else:\n",
        "        selected_variations = all_variations[:num_samples]\n",
        "    \n",
        "    return generate_images(\n",
        "        pipeline,\n",
        "        selected_variations,\n",
        "        output_dir,\n",
        "        num_images_per_prompt=num_images_per_prompt,\n",
        "        seed=seed\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurations + Load Model + Test with presonalized prompt\n",
        "\n",
        "base_model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "lora_weights_path = \"/content/drive/MyDrive/outputs_master_ProfAI/lora_model/pytorch_lora_weights.safetensors\"\n",
        "variations_file = \"/content/drive/MyDrive/outputs_master_ProfAI/captions/flan_t5_variations.json\"\n",
        "output_dir = \"/content/drive/MyDrive/outputs_master_ProfAI/generated_images\"\n",
        "\n",
        "pipeline = load_lora_model(\n",
        "    base_model_id=base_model_id,\n",
        "    lora_weights_path=lora_weights_path\n",
        ")\n",
        "\n",
        "test_prompts = [\n",
        "    \"A high-quality photograph of a British Shorthair cat sitting on a windowsill\",\n",
        "    \"A detailed image of a Beagle dog playing in a park\",\n",
        "    \"A professional photo of a Maine Coon cat with fluffy fur\"\n",
        "]\n",
        "\n",
        "generate_images(\n",
        "    pipeline,\n",
        "    test_prompts,\n",
        "    output_dir=output_dir + \"/test_prompts\",\n",
        "    num_images_per_prompt=1,\n",
        "    seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate images from variations previously generated \n",
        "\n",
        "generate_from_variations(\n",
        "    pipeline,\n",
        "    variations_file,\n",
        "    output_dir=output_dir + \"/variations\",\n",
        "    num_samples=10,\n",
        "    num_images_per_prompt=1,\n",
        "    seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unique Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "import random\n",
        "import shutil\n",
        "import subprocess\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Union, Optional, Tuple, Any\n",
        "from diffusers import (\n",
        "    StableDiffusionPipeline, \n",
        "    StableDiffusionXLPipeline,\n",
        "    DPMSolverMultistepScheduler,\n",
        "    DDPMScheduler,\n",
        "    AutoPipelineForText2Image, \n",
        "    FluxPipeline\n",
        ")\n",
        "\n",
        "class DiffusionModelManager:\n",
        "    \"\"\"\n",
        "    Unified manager class for diffusion models that handles:\n",
        "    - Zero-shot testing with different models\n",
        "    - Dataset preparation for LoRA fine-tuning\n",
        "    - LoRA fine-tuning process\n",
        "    - Inference with fine-tuned models\n",
        "    - Image generation from text variations\n",
        "    \n",
        "    This class centralizes all functionality related to diffusion models\n",
        "    in a single interface for easier workflow management.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        base_models_dir: Optional[str] = None,\n",
        "        output_dir: str = \"diffusion_output\",\n",
        "        device: str = \"cuda\",\n",
        "        default_model: str = \"runwayml/stable-diffusion-v1-5\"\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the diffusion model manager.\n",
        "        \n",
        "        Args:\n",
        "            base_models_dir: Directory to store cached models\n",
        "            output_dir: Directory for outputs (images, logs, checkpoints)\n",
        "            device: Device to use for inference/training (cuda or cpu)\n",
        "            default_model: Default model ID to use\n",
        "        \"\"\"\n",
        "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        self.models_dir = Path(base_models_dir) if base_models_dir else None\n",
        "        self.default_model = default_model\n",
        "\n",
        "        self.current_pipeline = None\n",
        "        self.current_model_id = None\n",
        "        \n",
        "        (self.output_dir / \"zero_shot\").mkdir(exist_ok=True)\n",
        "        (self.output_dir / \"fine_tuned\").mkdir(exist_ok=True)\n",
        "        (self.output_dir / \"datasets\").mkdir(exist_ok=True)\n",
        "        (self.output_dir / \"lora_models\").mkdir(exist_ok=True)\n",
        "        \n",
        "        print(f\"DiffusionModelManager initialized with device: {self.device}\")\n",
        "        print(f\"Output directory: {self.output_dir}\")\n",
        "\n",
        "    def test_diffusion_model(\n",
        "        self,\n",
        "        model_id: str,\n",
        "        caption: str,\n",
        "        num_images: int = 1,\n",
        "        seed: Optional[int] = None,\n",
        "        output_subdir: Optional[str] = None,\n",
        "        guidance_scale: float = 7.5,\n",
        "        num_inference_steps: int = 30,\n",
        "        width: int = 512,\n",
        "        height: int = 512,\n",
        "        save_images: bool = True\n",
        "    ) -> List[Image.Image]:\n",
        "        \"\"\"\n",
        "        Test a diffusion model with a specific caption in zero-shot setting.\n",
        "        \n",
        "        Args:\n",
        "            model_id: Hugging Face model ID\n",
        "            caption: Text caption for image generation\n",
        "            num_images: Number of images to generate\n",
        "            seed: Random seed for reproducibility\n",
        "            output_subdir: Subdirectory to save images\n",
        "            guidance_scale: Classifier-free guidance scale\n",
        "            num_inference_steps: Number of denoising steps\n",
        "            width: Image width\n",
        "            height: Image height\n",
        "            save_images: Whether to save generated images\n",
        "            \n",
        "        Returns:\n",
        "            List of generated PIL images\n",
        "        \"\"\"\n",
        "        print(f\"Testing model: {model_id}\")\n",
        "\n",
        "        if \"xl\" in model_id.lower():\n",
        "            pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "                model_id, \n",
        "                torch_dtype=torch.float16, \n",
        "                use_safetensors=True, \n",
        "                variant=\"fp16\"\n",
        "            )\n",
        "        elif \"kandinsky\" in model_id.lower():\n",
        "            pipe = AutoPipelineForText2Image.from_pretrained(\n",
        "                        \"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16\n",
        "                    ).to(\"cuda\")\n",
        "        elif \"flux\" in model_id.lower():\n",
        "            pipe = FluxPipeline.from_pretrained(\n",
        "                \"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16 \n",
        "            )\n",
        "        else:\n",
        "            pipe = StableDiffusionPipeline.from_pretrained(\n",
        "                model_id,\n",
        "                torch_dtype=torch.float16\n",
        "            )\n",
        "        \n",
        "        pipe = pipe.to(\"cuda\")\n",
        "\n",
        "        # Save memory\n",
        "        pipe.enable_attention_slicing()\n",
        "        if hasattr(pipe, 'enable_vae_slicing'):\n",
        "            pipe.enable_vae_slicing()\n",
        "\n",
        "        breed_info = \"\"\n",
        "        if \" - This is \" in caption[0]:\n",
        "            parts = caption[0].split(\" - This is a \")\n",
        "            cleaned_caption = parts[0]\n",
        "            breed_info = parts[1].strip(\".\")\n",
        "            prompt = f\"A high-quality photo of a {breed_info}, {cleaned_caption}\"\n",
        "        else:\n",
        "            prompt = f\"A high-quality photo of {caption}\"\n",
        "\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "\n",
        "        images = []\n",
        "        for i in range(num_images):\n",
        "            generator = None\n",
        "            if seed is not None:\n",
        "                generator = torch.Generator(device = \"cuda\").manual_seed(seed + i)\n",
        "\n",
        "            if \"kandinsky\" in model_id.lower():\n",
        "                image = pipe(prompt, generator = generator).images[0]\n",
        "            elif \"flux\" in model_id.lower():\n",
        "                image = pipe(prompt,\n",
        "                             guidance_scale=guidance_scale,\n",
        "                            num_inference_steps=num_inference_steps,\n",
        "                            generator=generator,\n",
        "                            width=width,\n",
        "                            height=height,\n",
        "                            max_sequence_length=512\n",
        "                            ).images[0]\n",
        "            else:\n",
        "                image = pipe(\n",
        "                    prompt, \n",
        "                    guidance_scale=guidance_scale,\n",
        "                    num_inference_steps=num_inference_steps,\n",
        "                    generator=generator\n",
        "                ).images[0]\n",
        "            \n",
        "            images.append(image)\n",
        "\n",
        "        if save_images:\n",
        "            save_dir = self.output_dir / \"zero_shot\"\n",
        "            if output_subdir:\n",
        "                save_dir = save_dir / output_subdir\n",
        "                \n",
        "            save_dir.mkdir(parents=True, exist_ok=True)\n",
        "            \n",
        "            model_name = model_id.split(\"/\")[-1]\n",
        "            for i, img in enumerate(images):\n",
        "                img_path = save_dir / f\"{model_name}_{i}_seed{seed}.png\"\n",
        "                img.save(img_path)\n",
        "                \n",
        "                with open(save_dir / f\"{model_name}_{i}_seed{seed}.txt\", \"w\") as f:\n",
        "                    f.write(prompt)\n",
        "\n",
        "        fig, axes = plt.subplots(1, num_images, figsize=(5*num_images, 5))\n",
        "        if num_images == 1:\n",
        "            axes = [axes]\n",
        "        \n",
        "        for i, img in enumerate(images):\n",
        "            axes[i].imshow(np.array(img))\n",
        "            axes[i].set_title(f\"Image {i+1}\")\n",
        "            axes[i].axis(\"off\")\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return images\n",
        "\n",
        "    def prepare_dataset(\n",
        "        self,\n",
        "        captions_file: Union[str, Path],\n",
        "        images_dir: Union[str, Path],\n",
        "        output_name: Optional[str] = None,\n",
        "        max_samples_per_breed: Optional[int] = None,\n",
        "        min_samples_per_breed: int = 3,\n",
        "        target_total_samples: Optional[int] = None,\n",
        "        class_field: str = \"breed\",\n",
        "        resolution: int = 512\n",
        "    ) -> Path:\n",
        "        \"\"\"\n",
        "        Prepare a dataset for LoRA fine-tuning.\n",
        "        \n",
        "        Args:\n",
        "            captions_file: Path to JSON file with captions\n",
        "            images_dir: Directory containing images\n",
        "            output_name: Name of output dataset directory\n",
        "            max_samples_per_breed: Maximum samples per class\n",
        "            min_samples_per_breed: Minimum samples per class\n",
        "            target_total_samples: Target total number of samples\n",
        "            class_field: Field name for class information extraction\n",
        "            resolution: Target resolution for images\n",
        "            \n",
        "        Returns:\n",
        "            Path to prepared dataset\n",
        "        \"\"\"\n",
        "        if output_name:\n",
        "            output_dir = self.output_dir / \"datasets\" / output_name\n",
        "        else:\n",
        "            timestamp = Path(captions_file).stem\n",
        "            output_dir = self.output_dir / \"datasets\" / f\"dataset_{timestamp}\"\n",
        "            \n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        with open(captions_file, \"r\") as f:\n",
        "            captions = json.load(f)\n",
        "                \n",
        "        breed_samples = {}\n",
        "        for img_path, caption in captions.items():\n",
        "            if \" - This is a \" in caption:\n",
        "                breed = caption.split(\" - This is a \")[1].strip(\".\")\n",
        "\n",
        "                if not breed:\n",
        "                    continue\n",
        "\n",
        "                if breed not in breed_samples:\n",
        "                    breed_samples[breed] = []\n",
        "                \n",
        "                img_name = Path(img_path).name\n",
        "                full_img_path = Path(images_dir) / img_name\n",
        "\n",
        "                if full_img_path.exists():\n",
        "                    breed_samples[breed].append((str(full_img_path), caption))\n",
        "                else:\n",
        "                    print(f\"Warning: Image not found: {img_path}\")\n",
        "                    continue\n",
        "\n",
        "        \n",
        "        # Select a balanced subset\n",
        "        selected_samples = []\n",
        "        metadata = []\n",
        "        \n",
        "        for breed, samples in breed_samples.items():\n",
        "            num_samples = min(\n",
        "                len(samples),\n",
        "                max_samples_per_breed if max_samples_per_breed else len(samples)\n",
        "            )\n",
        "            num_samples = max(num_samples, min_samples_per_breed)\n",
        "            \n",
        "            # Select random sample\n",
        "            breed_selection = random.sample(samples, min(num_samples, len(samples)))\n",
        "            selected_samples.extend(breed_selection)\n",
        "        \n",
        "        # Limit with target_total_samples\n",
        "        if target_total_samples and len(selected_samples) > target_total_samples:\n",
        "            random.shuffle(selected_samples)\n",
        "            selected_samples = selected_samples[:target_total_samples]\n",
        "        \n",
        "        print(f\"Selected {len(selected_samples)} samples from {len(breed_samples)} breed\")\n",
        "        \n",
        "        # Create a metadata.jsonl as required by LoRA\n",
        "        for i, (img_path, caption) in enumerate(selected_samples):\n",
        "            dest_filename = f\"image_{i:06d}.jpg\"\n",
        "            dest_path = output_dir / dest_filename\n",
        "            #shutil.copy(img_path, dest_path)\n",
        "\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "            \n",
        "            # Resize if needed\n",
        "            if resolution:\n",
        "                # Center crop to square while maintaining aspect ratio\n",
        "                width, height = img.size\n",
        "                min_dim = min(width, height)\n",
        "                left = (width - min_dim) // 2\n",
        "                top = (height - min_dim) // 2\n",
        "                right = left + min_dim\n",
        "                bottom = top + min_dim\n",
        "                \n",
        "                img = img.crop((left, top, right, bottom))\n",
        "                img = img.resize((resolution, resolution), Image.LANCZOS)\n",
        "                \n",
        "            # Save image\n",
        "            img.save(dest_path)\n",
        "            \n",
        "            metadata.append({\n",
        "                \"file_name\": dest_filename,\n",
        "                \"text\": caption\n",
        "            })\n",
        "        \n",
        "        metadata_path = output_dir / \"metadata.jsonl\"\n",
        "        with open(metadata_path, \"w\") as f:\n",
        "            for item in metadata:\n",
        "                f.write(json.dumps(item) + \"\\n\")\n",
        "        \n",
        "        print(f\"Dataset prepared in {output_dir}\")\n",
        "        print(f\"Metadata saved to: {metadata_path}\")\n",
        "\n",
        "        return output_dir\n",
        "\n",
        "    def select_validation_prompts_from_variations(\n",
        "        self, \n",
        "        variations_file: Union[str, Path], \n",
        "        num_prompts: int = 5, \n",
        "        seed: int = 42\n",
        "    ) -> List[str]:\n",
        "        \"\"\"\n",
        "        Select validation prompts from a variations file.\n",
        "        \n",
        "        Args:\n",
        "            variations_file: Path to variations JSON file\n",
        "            num_prompts: Number of prompts to select\n",
        "            seed: Random seed for selection\n",
        "            \n",
        "        Returns:\n",
        "            List of selected prompts\n",
        "        \"\"\"\n",
        "        with open(variations_file, 'r') as f:\n",
        "            variations = json.load(f)\n",
        "        \n",
        "        all_captions = []\n",
        "        for variations_list in variations.values():\n",
        "            all_captions.extend(variations_list)\n",
        "        \n",
        "        random.seed(seed)\n",
        "        selected_prompts = random.sample(all_captions, min(num_prompts, len(all_captions)))\n",
        "        \n",
        "        return selected_prompts\n",
        "\n",
        "    def run_lora_training(\n",
        "        self,\n",
        "        dataset_dir: Union[str, Path],\n",
        "        output_name: Optional[str] = None,\n",
        "        base_model: Optional[str] = None,\n",
        "        resolution: int = 512,\n",
        "        train_batch_size: int = 1,\n",
        "        max_train_steps: int = 1000,\n",
        "        learning_rate: float = 1e-4,\n",
        "        validation_prompts: Optional[List[str]] = None,\n",
        "        rank: int = 4\n",
        "    ) -> Path:\n",
        "        \"\"\"\n",
        "        Run LoRA fine-tuning on a prepared dataset.\n",
        "        \n",
        "        Args:\n",
        "            dataset_dir: Directory with prepared dataset\n",
        "            output_name: Name for the output directory\n",
        "            base_model: Base model ID\n",
        "            resolution: Image resolution\n",
        "            train_batch_size: Batch size for training\n",
        "            max_train_steps: Maximum training steps\n",
        "            learning_rate: Learning rate\n",
        "            validation_prompts: Prompts for validation\n",
        "            rank: LoRA rank parameter\n",
        "            lora_alpha: LoRA alpha parameter\n",
        "            \n",
        "        Returns:\n",
        "            Path to fine-tuned model\n",
        "        \"\"\"\n",
        "        dataset_dir = Path(dataset_dir)\n",
        "        \n",
        "        if output_name:\n",
        "            output_dir = self.output_dir / \"lora_models\" / output_name\n",
        "        else:\n",
        "            timestamp = dataset_dir.name\n",
        "            output_dir = self.output_dir / \"lora_models\" / f\"lora_{timestamp}\"\n",
        "            \n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        if base_model is None:\n",
        "            base_model = self.default_model\n",
        "            \n",
        "        print(f\"Starting LoRA training with base model: {base_model}\")\n",
        "        print(f\"Dataset: {dataset_dir}\")\n",
        "        print(f\"Output directory: {output_dir}\")\n",
        "\n",
        "        # Correct diffusers version if you didn't run it before\n",
        "        try:\n",
        "            subprocess.run([\"pip\", \"install\", \"-q\", \"git+https://github.com/huggingface/diffusers.git\"])\n",
        "            subprocess.run([\"pip\", \"install\", \"-q\", \"accelerate\", \"transformers\", \"bitsandbytes\", \"datasets\"])\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Failed to install dependencies - {e}\")\n",
        "            \n",
        "        # We need to locally download the most up-to-date version of train_text_to_image_lora.py\n",
        "        script_path = Path.cwd() / \"train_text_to_image_lora.py\"\n",
        "        try:\n",
        "            subprocess.run([\n",
        "                \"wget\", \n",
        "                \"https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/train_text_to_image_lora.py\", \n",
        "                \"-O\", \n",
        "                str(script_path)\n",
        "                # \"train_text_to_image_lora.py\"\n",
        "            ])\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Failed to download script - {e}\")\n",
        "            if not script_path.exists():\n",
        "                raise RuntimeError(\"Could not download training script and no local script found.\")\n",
        "                \n",
        "        if validation_prompts is None:\n",
        "            validation_prompts = [\n",
        "                \"A high-quality photo of a dog\",\n",
        "                \"A high-quality photo of a cat\",\n",
        "                \"A close-up portrait of a pet\"\n",
        "            ]\n",
        "\n",
        "        # Command to use train_text_to_image_lora.py\n",
        "        cmd = [\n",
        "            \"accelerate\", \"launch\",\n",
        "            \"train_text_to_image_lora.py\",\n",
        "            f\"--pretrained_model_name_or_path={base_model}\",\n",
        "            f\"--train_data_dir={dataset_dir}\",\n",
        "            f\"--output_dir={output_dir}\",\n",
        "            f\"--resolution={resolution}\",\n",
        "            \"--center_crop\",\n",
        "            \"--random_flip\",\n",
        "            f\"--train_batch_size={train_batch_size}\",\n",
        "            \"--gradient_accumulation_steps=4\",\n",
        "            \"--gradient_checkpointing\",\n",
        "            \"--mixed_precision=fp16\",\n",
        "            f\"--max_train_steps={max_train_steps}\",\n",
        "            f\"--learning_rate={learning_rate}\",\n",
        "            \"--lr_scheduler=constant\",\n",
        "            \"--lr_warmup_steps=0\",\n",
        "            \"--validation_epochs=100\",\n",
        "            f\"--validation_prompt=\\\"{'; '.join(validation_prompts)}\\\"\",\n",
        "            \"--seed=42\",\n",
        "            \"--checkpointing_steps=500\",\n",
        "            f\"--rank={rank}\"\n",
        "        ]\n",
        "\n",
        "        print(f\"Running command: {' '.join(cmd)}\")\n",
        "        try:\n",
        "            process = subprocess.run(cmd, capture_output=True, text=True)\n",
        "            # Log output\n",
        "            with open(output_dir / \"training_log.txt\", \"w\") as f:\n",
        "                f.write(f\"STDOUT:\\n{process.stdout}\\n\\nSTDERR:\\n{process.stderr}\")\n",
        "                \n",
        "            if process.returncode != 0:\n",
        "                print(f\"Warning: Training process exited with code {process.returncode}\")\n",
        "                print(f\"Error details: {process.stderr}\")\n",
        "            else:\n",
        "                print(f\"Training completed successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during training: {e}\")\n",
        "            raise\n",
        "            \n",
        "        return output_dir\n",
        "\n",
        "    def load_lora_model(\n",
        "        self,\n",
        "        base_model_id: Optional[str] = None,\n",
        "        lora_weights_path: Optional[str] = None,\n",
        "        torch_dtype = torch.float16\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Load a model with LoRA weights.\n",
        "        \n",
        "        Args:\n",
        "            base_model_id: ID of the base model\n",
        "            lora_weights_path: Path to LoRA weights\n",
        "            torch_dtype: Data type for model loading\n",
        "            \n",
        "        Returns:\n",
        "            self for method chaining\n",
        "        \"\"\"\n",
        "        if base_model_id is None:\n",
        "            base_model_id = self.default_model\n",
        "            \n",
        "        print(f\"Loading base model: {base_model_id}\")\n",
        "        \n",
        "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "            base_model_id,\n",
        "            torch_dtype=torch_dtype\n",
        "        )\n",
        "        \n",
        "        pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
        "        \n",
        "        if lora_weights_path:\n",
        "            print(f\"Loading LoRA weights from: {lora_weights_path}\")\n",
        "            pipeline.unet.load_attn_procs(lora_weights_path)\n",
        "            #pipeline.load_lora_adapter(lora_weights_path)\n",
        "            print(\"LoRA weights loaded successfully!\")\n",
        "        \n",
        "        pipeline.to(self.device)\n",
        "        pipeline.enable_attention_slicing()\n",
        "        \n",
        "        self.current_pipeline = pipeline\n",
        "        self.current_model_id = base_model_id\n",
        "        \n",
        "        return self\n",
        "\n",
        "    def generate_images(\n",
        "        self,\n",
        "        prompts: Union[str, List[str]],\n",
        "        output_subdir: Optional[str] = None,\n",
        "        num_images_per_prompt: int = 1,\n",
        "        guidance_scale: float = 7.5,\n",
        "        num_inference_steps: int = 30,\n",
        "        seed: Optional[int] = None,\n",
        "        width: int = 512,\n",
        "        height: int = 512,\n",
        "        save_images: bool = True,\n",
        "        display_images: bool = True,\n",
        "        batch_size: int = 1,\n",
        "        negative_prompt: Optional[str] = None\n",
        "    ) -> Dict[str, List[Image.Image]]:\n",
        "        \"\"\"\n",
        "        Generate images using the current model pipeline.\n",
        "        \n",
        "        Args:\n",
        "            prompts: Text prompts for image generation\n",
        "            output_subdir: Subdirectory to save images\n",
        "            num_images_per_prompt: Number of images per prompt\n",
        "            guidance_scale: Guidance scale for classifier-free guidance\n",
        "            num_inference_steps: Number of inference steps\n",
        "            seed: Random seed for reproducibility\n",
        "            width: Image width\n",
        "            height: Image height\n",
        "            save_images: Whether to save generated images\n",
        "            display_images: Whether to display generated images\n",
        "            batch_size: Batch size for generation\n",
        "            negative_prompt: Negative prompt for generation\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary mapping prompts to generated images\n",
        "        \"\"\"\n",
        "        if self.current_pipeline is None:\n",
        "            self.load_lora_model()\n",
        "            \n",
        "        if isinstance(prompts, str):\n",
        "            prompts = [prompts]\n",
        "            \n",
        "        if save_images:\n",
        "            if output_subdir:\n",
        "                output_path = self.output_dir / \"fine_tuned\" / output_subdir\n",
        "            else:\n",
        "                output_path = self.output_dir / \"fine_tuned\" / \"generated\"\n",
        "                \n",
        "            output_path.mkdir(parents=True, exist_ok=True)\n",
        "            \n",
        "        results = {}\n",
        "        all_images = []\n",
        "        all_prompts = []\n",
        "        \n",
        "        for i in range(0, len(prompts), batch_size):\n",
        "            batch_prompts = prompts[i:i + batch_size]\n",
        "            \n",
        "            generator = None\n",
        "            if seed is not None:\n",
        "                generator = torch.Generator(device=self.device).manual_seed(seed)\n",
        "                seed += 1\n",
        "                \n",
        "            print(f\"Generating batch {i//batch_size + 1}/{(len(prompts)-1)//batch_size + 1}...\")\n",
        "            \n",
        "            batch_results = self.current_pipeline(\n",
        "                batch_prompts,\n",
        "                num_images_per_prompt=num_images_per_prompt,\n",
        "                guidance_scale=guidance_scale,\n",
        "                num_inference_steps=num_inference_steps,\n",
        "                generator=generator,\n",
        "                #negative_prompt=negative_prompt,\n",
        "                width=width,\n",
        "                height=height\n",
        "            )\n",
        "            \n",
        "            batch_images = batch_results.images\n",
        "            \n",
        "            for j, prompt in enumerate(batch_prompts):\n",
        "                start_idx = j * num_images_per_prompt\n",
        "                end_idx = start_idx + num_images_per_prompt\n",
        "                prompt_images = batch_images[start_idx:end_idx]\n",
        "                \n",
        "                results[prompt] = prompt_images\n",
        "                all_images.extend(prompt_images)\n",
        "                all_prompts.extend([prompt] * num_images_per_prompt)\n",
        "                \n",
        "                if save_images:\n",
        "                    for k, img in enumerate(prompt_images):\n",
        "                        prompt_hash = abs(hash(prompt)) % 10000\n",
        "                        img_path = output_path / f\"gen_{prompt_hash}_{i}_{j}_{k}.png\"\n",
        "                        img.save(img_path)\n",
        "                        \n",
        "                        with open(output_path / f\"gen_{prompt_hash}_{i}_{j}_{k}.txt\", \"w\") as f:\n",
        "                            f.write(prompt)\n",
        "        \n",
        "        if display_images and all_images:\n",
        "            n_cols = min(5, len(all_images))\n",
        "            n_rows = (len(all_images) + n_cols - 1) // n_cols\n",
        "            \n",
        "            plt.figure(figsize=(n_cols * 5, n_rows * 6))\n",
        "            \n",
        "            for i, (image, prompt) in enumerate(zip(all_images, all_prompts)):\n",
        "                plt.subplot(n_rows, n_cols, i + 1)\n",
        "                plt.imshow(np.array(image))\n",
        "                plt.title(prompt[:50] + \"...\" if len(prompt) > 50 else prompt, fontsize=8)\n",
        "                plt.axis(\"off\")\n",
        "                \n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            \n",
        "        return results\n",
        "    \n",
        "\n",
        "    def generate_from_variations(\n",
        "        self,\n",
        "        variations_file: Union[str, Path],\n",
        "        output_subdir: Optional[str] = None,\n",
        "        num_samples: int = 5,\n",
        "        num_images_per_prompt: int = 1,\n",
        "        guidance_scale: float = 7.5,\n",
        "        num_inference_steps: int = 30,\n",
        "        seed: Optional[int] = None,\n",
        "        random_variations: bool = True,\n",
        "        negative_prompt: Optional[str] = None\n",
        "    ) -> Dict[str, List[Image.Image]]:\n",
        "        \"\"\"\n",
        "        Generate images from text variations in a file.\n",
        "        \n",
        "        Args:\n",
        "            variations_file: JSON file with text variations\n",
        "            output_subdir: Subdirectory for saving images\n",
        "            num_samples: Number of variation prompts to use\n",
        "            num_images_per_prompt: Number of images per prompt\n",
        "            guidance_scale: Guidance scale for generation\n",
        "            num_inference_steps: Number of inference steps\n",
        "            seed: Random seed for reproducibility\n",
        "            random_variations: Whether to sample variations randomly\n",
        "            negative_prompt: Negative prompt for generation\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary mapping prompts to generated images\n",
        "        \"\"\"\n",
        "        with open(variations_file, \"r\") as f:\n",
        "            variations_data = json.load(f)\n",
        "            \n",
        "        all_variations = []\n",
        "        for img_path, variations in variations_data.items():\n",
        "            all_variations.extend(variations)\n",
        "            \n",
        "        if random_variations:\n",
        "            selected_variations = random.sample(all_variations, min(num_samples, len(all_variations)))\n",
        "        else:\n",
        "            selected_variations = all_variations[:num_samples]\n",
        "            \n",
        "        return self.generate_images(\n",
        "            prompts=selected_variations,\n",
        "            output_subdir=output_subdir or \"variations\",\n",
        "            num_images_per_prompt=num_images_per_prompt,\n",
        "            guidance_scale=guidance_scale,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            seed=seed,\n",
        "            #negative_prompt=negative_prompt\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Diffusion class\n",
        "\n",
        "diffusion_manager = DiffusionModelManager(\n",
        "    output_dir=\"/content/drive/MyDrive/outputs_master_ProfAI\",\n",
        "    default_model=\"runwayml/stable-diffusion-v1-5\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Zero-Shot Prompting\n",
        "\n",
        "models_to_test = [\n",
        "    \"stabilityai/stable-diffusion-2-1-base\",\n",
        "    #\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    \"kandinsky-community/kandinsky-2-2-decoder\",\n",
        "    \"black-forest-labs/FLUX.1-dev\"\n",
        "]\n",
        "\n",
        "for model in models_to_test:\n",
        "    diffusion_manager.test_diffusion_model(\n",
        "        model_id=model,\n",
        "        prompt=\"A high-quality photo of a British Shorthair cat\",\n",
        "        num_images=2,\n",
        "        output_subdir=\"model_comparison\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "caption_file = Path('/content/drive/MyDrive/outputs_master_ProfAI/captions/flan_t5_variations.json')\n",
        "with open(caption_file, 'r') as f:\n",
        "    balanced_captions = json.load(f)\n",
        "\n",
        "sample_captions = random.sample(list(balanced_captions.values()), 3)\n",
        "print(\"Caption selected for this test:\")\n",
        "for i, caption in enumerate(sample_captions):\n",
        "    print(f\"{i+1}. {caption}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for caption in sample_captions:\n",
        "    diffusion_manager.test_diffusion_model(models_to_test[0], caption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "captions_file = \"/content/drive/MyDrive/outputs_master_ProfAI/captions/captions_git_train_dataset.json\"\n",
        "images_dir = \"/content/Gen-AI-for-Data-Augmentation/data/oxford-iiit-pet/images\"\n",
        "\n",
        "dataset_dir = diffusion_manager.prepare_dataset(\n",
        "    captions_file=captions_file,\n",
        "    images_dir=images_dir,\n",
        "    max_samples_per_breed=20,\n",
        "    min_samples_per_breed=10,\n",
        "    target_total_samples=37*20,\n",
        "    resolution=512\n",
        ")\n",
        "\n",
        "variations_file = \"/content/drive/MyDrive/outputs_master_ProfAI/captions/flan_t5_variations.json\"\n",
        "validation_prompts = diffusion_manager.select_validation_prompts_from_variations(\n",
        "    variations_file=variations_file,\n",
        "    num_prompts=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run LoRA fine-tuning\n",
        "\n",
        "lora_model_dir = diffusion_manager.run_lora_training(\n",
        "    dataset_dir=dataset_dir,\n",
        "    output_name=\"pet_breeds_lora\",\n",
        "    base_model=\"runwayml/stable-diffusion-v1-5\",\n",
        "    resolution=512,\n",
        "    max_train_steps=1500,\n",
        "    learning_rate=5e-5,\n",
        "    validation_prompts=validation_prompts,\n",
        "    rank=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the fine-tuned model\n",
        "\n",
        "diffusion_manager.load_lora_model(\n",
        "    base_model_id=\"runwayml/stable-diffusion-v1-5\",\n",
        "    lora_weights_path=str(lora_model_dir / \"pytorch_lora_weights.safetensors\")\n",
        ")\n",
        "\n",
        "# Generate images with custom prompts\n",
        "test_prompts = [\n",
        "    \"A high-quality photograph of a Maine Coon cat with long fur\",\n",
        "    \"A detailed image of a Beagle dog running in a park\",\n",
        "    \"A professional photo of a Persian cat with blue eyes\"\n",
        "]\n",
        "\n",
        "generated_images = diffusion_manager.generate_images(\n",
        "    prompts=test_prompts,\n",
        "    output_subdir=\"test_generation\",\n",
        "    num_images_per_prompt=2,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=40,\n",
        "    negative_prompt=\"blurry, deformed, distorted, low quality, poor details, watermark\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate images from variations\n",
        "\n",
        "variation_images = diffusion_manager.generate_from_variations(\n",
        "    variations_file=variations_file,\n",
        "    output_subdir=\"variation_generation\",\n",
        "    num_samples=20,\n",
        "    num_images_per_prompt=1,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=40,\n",
        "    negative_prompt=\"blurry, deformed, distorted, low quality, poor details, watermark\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUUthrS4Ac3h"
      },
      "source": [
        "## Imgae Generation with Conditional GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oY6xFDKYAc3h",
        "outputId": "e247399b-4a59-4963-da0e-3a8002639797"
      },
      "outputs": [],
      "source": [
        "# Setup device\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device used: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WzBPGaG_Ac3i"
      },
      "outputs": [],
      "source": [
        "# Configurations\n",
        "\n",
        "batch_size = 32\n",
        "image_size = 128\n",
        "num_workers = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnPm2yGnAc3i",
        "outputId": "cc737556-9022-4c98-8f2e-81a1d8fb4a5c"
      },
      "outputs": [],
      "source": [
        "# Setup components\n",
        "\n",
        "output_dir = Path(\"/content/drive/MyDrive/outputs_master_ProfAI\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "checkpoint_dir = output_dir / \"checkpoints\"\n",
        "log_dir = output_dir / \"logs\"\n",
        "\n",
        "metrics = MetricsTracker([\n",
        "    FIDScore(device=device),\n",
        "    CLIPScore(device=device)\n",
        "])\n",
        "\n",
        "logger = GANLogger(\"conditional_gan\", log_dir=log_dir)\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='fid', patience=5),\n",
        "    ModelCheckpoint(filepath=checkpoint_dir / \"best_model.pt\", monitor='fid'),\n",
        "    MetricsHistory(log_dir=log_dir / \"metrics\")\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "I0yjVD0295yM"
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "\n",
        "data_dir = Path('./data')\n",
        "handler = PetDatasetHandler(data_dir)\n",
        "train_dataset, test_dataset = handler.load_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "r6mtOOHp95yN"
      },
      "outputs": [],
      "source": [
        "# Load Captions\n",
        "with open('output/captions/captions_traindataset.json', 'r') as f:\n",
        "    caption_dict = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "TRoTLwvp95yN"
      },
      "outputs": [],
      "source": [
        "train_images_paths = [str(img) for img in train_dataset._images]\n",
        "\n",
        "test_images_paths = [str(img) for img in test_dataset._images]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thgLwkV0Ac3j",
        "outputId": "09f6f851-4fe2-4e56-9030-b8d027e9ad8b"
      },
      "outputs": [],
      "source": [
        "# Initialize train and val loader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "full_train_ds = PetDatasetWithCaptions(\n",
        "    image_paths=train_images_paths,\n",
        "    caption_dict=caption_dict,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_size = int(0.9 * len(full_train_ds))\n",
        "val_size = len(full_train_ds) - train_size\n",
        "train_ds, val_ds = random_split(full_train_ds, [train_size, val_size],\n",
        "                                generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "test_ds = PetDatasetWithCaptions(\n",
        "    image_paths=test_images_paths,\n",
        "    caption_dict=caption_dict,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PId4e9AMAc3k",
        "outputId": "6a9eb72d-3518-4cf8-fb2e-29d4fb3a7977"
      },
      "outputs": [],
      "source": [
        "# Initialize GAN model\n",
        "\n",
        "config = GANConfig(\n",
        "    latent_dim = 100,\n",
        "    caption_dim = 768,\n",
        "    image_size = image_size,\n",
        "    num_channels = 3,\n",
        "    generator_features = 64\n",
        ")\n",
        "\n",
        "gan = ConditionalGAN(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "eJwz9DZ-Ac3k"
      },
      "outputs": [],
      "source": [
        "# Initialize trainer\n",
        "\n",
        "trainer = GANTrainer(\n",
        "    gan=gan,\n",
        "    train_dataloader=train_loader,\n",
        "    val_dataloader=val_loader,\n",
        "    metrics_tracker=metrics,\n",
        "    logger=logger,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlyZmIyNAc3k",
        "outputId": "04f48ba7-5b95-4f12-8986-48c8929877c2"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "\n",
        "trainer.train(\n",
        "    num_epochs=100,\n",
        "    eval_freq=1,\n",
        "    sample_freq=500,\n",
        "    sample_dir=Path(\"samples\")\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
